{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bert_score\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bleurt = evaluate.load(\"bleurt\", module_type='metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "def icl_results(icl_method, ds_name, model_name):\n",
    "    results_data=[]\n",
    "    with open(f'icl_results/outputs/{icl_method}_{ds_name}_{model_name}.txt', 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        print(len(lines))\n",
    "        i=0\n",
    "        while i < len(lines)-1: \n",
    "            preds = ast.literal_eval(lines[i+1][11:])\n",
    "            reals = ast.literal_eval(lines[i][11:])\n",
    "            print(reals)\n",
    "            print(preds)\n",
    "            P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "            average_F1 = sum(F1) / len(F1)\n",
    "            # bert_scores.append(average_F1)\n",
    "            refs = [[r] for r in reals]\n",
    "            bleu_score = bleu.compute(predictions=preds, references=refs, max_order=1)\n",
    "            bleu_score2 = bleu.compute(predictions=preds, references=refs, max_order = 2)\n",
    "            bleu_score4 = bleu.compute(predictions=preds, references=refs)\n",
    "\n",
    "            # accuracy = 0\n",
    "            # for r,p in zip(reals, preds):\n",
    "            #     if len(p.strip()) != 0:\n",
    "            #         if r.strip()[0] == p.strip()[0]:\n",
    "            #             accuracy+=1\n",
    "            # accuracy = accuracy/len(preds)\n",
    "            # print(\"ACCURACY (comparing first letter): \", accuracy)\n",
    "\n",
    "            # accuracy = 0\n",
    "            # for r,p in zip(reals, preds):\n",
    "            #     if p.strip()==r.strip():\n",
    "            #         accuracy+=1\n",
    "            # accuracy = accuracy/len(preds)\n",
    "            # print(\"ACCURACY (exact) \", accuracy)\n",
    "\n",
    "            bleurt_score = bleurt.compute(predictions=preds, references=reals)\n",
    "            avg_bleurt = sum(bleurt_score['scores'])/len(bleurt_score['scores'])\n",
    "            results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i//3, 'bert_score' : float(average_F1), 'bleu-1' : bleu_score['bleu'], 'bleu-2':bleu_score2['bleu'],\n",
    "                                'bleu-4':bleu_score4['bleu'], 'bleurt' : avg_bleurt})\n",
    "            i+=3\n",
    "\n",
    "    results_df1 = pd.DataFrame(results_data)\n",
    "    return results_df1\n",
    "\n",
    "def it_results(ds_name, model_name):\n",
    "\n",
    "    results_data = []\n",
    "\n",
    "    # Open the file containing the real values and predictions\n",
    "    with open(f'it_results/outputs/{ds_name}_{model_name}_it.txt', 'r', encoding='utf-8') as file:\n",
    "        # Read lines from the file\n",
    "        lines = file.readlines()\n",
    "        print(len(lines))\n",
    "        i=0\n",
    "        while i < len(lines)-1: # Assuming that every 3 lines correspond to one iteration\n",
    "            preds = ast.literal_eval(lines[i+1][11:])\n",
    "            reals = ast.literal_eval(lines[i][11:])\n",
    "            print(reals)\n",
    "            print(preds)\n",
    "            P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "            average_F1 = sum(F1) / len(F1)\n",
    "            # bert_scores.append(average_F1)\n",
    "            refs = [[r] for r in reals]\n",
    "            order = int(sum(len(s) for s in refs)/len(refs))\n",
    "            bleu_score = bleu.compute(predictions=preds, references=refs, max_order=1)\n",
    "            bleu_score2 = bleu.compute(predictions=preds, references=refs, max_order = 2)\n",
    "            bleu_score4 = bleu.compute(predictions=preds, references=refs)\n",
    "            # rouge_score = rouge.compute(predictions=preds, references=refs)\n",
    "            \n",
    "            # accuracy = 0\n",
    "            # for r,p in zip(reals, preds):\n",
    "            #     if len(p.strip()) != 0:\n",
    "            #         if r.strip()[0] == p.strip()[0]:\n",
    "            #             accuracy+=1\n",
    "            # accuracy = accuracy/len(preds)\n",
    "            # print(\"ACCURACY (comparing first letter): \", accuracy)\n",
    "\n",
    "            # accuracy = 0\n",
    "            # for r,p in zip(reals, preds):\n",
    "            #     if p.strip()==r.strip():\n",
    "            #         accuracy+=1\n",
    "            # accuracy = accuracy/len(preds)\n",
    "            # print(\"ACCURACY (exact) \", accuracy)\n",
    "\n",
    "            bleurt_score = bleurt.compute(predictions=preds, references=reals)\n",
    "            avg_bleurt = sum(bleurt_score['scores'])/len(bleurt_score['scores'])\n",
    "            results_data.append({'num_samples' : len(preds), 'num_demonstrations' : None, 'bert_score' : float(average_F1), 'bleu-1' : bleu_score['bleu'], 'bleu-2':bleu_score2['bleu'], \n",
    "                                'bleu-4':bleu_score4['bleu'], 'bleurt':avg_bleurt})\n",
    "            i+=3\n",
    "\n",
    "    results_df2 = pd.DataFrame(results_data)\n",
    "    return results_df2\n",
    "\n",
    "datasets = ['ni', 'alpaca', 'medmcq','finance_sent', 'medqa', 'lawqa']\n",
    "# icl_dfs = []\n",
    "# for d in datasets:\n",
    "#     df1 = icl_results('random', d, 'gpt2_small')\n",
    "#     df1['dataset'] = d\n",
    "#     icl_dfs.append(df1)\n",
    "# big_icl_df = pd.concat(icl_dfs, axis=0)\n",
    "\n",
    "icl_dfs = []\n",
    "for d in datasets:\n",
    "    df1 = icl_results('similarity', d, 'mistral')\n",
    "    df1['dataset'] = d\n",
    "    icl_dfs.append(df1)\n",
    "big_icl_df2 = pd.concat(icl_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_icl_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_icl_df.drop(['dataset'], axis=1).groupby('num_demonstrations').agg('mean').head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_icl_df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_icl_df2.drop(['dataset'], axis=1).groupby('num_demonstrations').agg('mean').head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "zero_shot = big_icl_df2[big_icl_df2['num_demonstrations'] == 0]\n",
    "one_shot = big_icl_df2[big_icl_df2['num_demonstrations'] == 1]\n",
    "two_shot = big_icl_df2[big_icl_df2['num_demonstrations'] == 2]\n",
    "\n",
    "\n",
    "big_icl_df2.loc[(big_icl_df2['dataset'] == 'finance_sent') & (big_icl_df2['num_demonstrations'] == 0), 'bleu-4'] = 0.0\n",
    "big_icl_df2.loc[(big_icl_df2['dataset'] == 'finance_sent') & (big_icl_df2['num_demonstrations'] == 1), 'bleu-4'] = 0.006\n",
    "big_icl_df2.loc[(big_icl_df2['dataset'] == 'finance_sent') & (big_icl_df2['num_demonstrations'] == 2), 'bleu-4'] = 0.006\n",
    "\n",
    "# big_icl_df2.loc[(big_icl_df2['dataset'] == 'medqa') & (big_icl_df2['num_demonstrations'] == 0) , 'bert_score'] = 0.0\n",
    "\n",
    "var = 'bleu-4'\n",
    "\n",
    "\n",
    "# Define 'it' scores\n",
    "# it = {'ni': 0.878, 'alpaca': 0.883, 'medmcq': 0.859, 'finance_sent':0.949, 'medqa': 0.851, 'lawqa': 0.859} #bert\n",
    "# # it = {'ni': 0.093, 'alpaca': 0.074, 'medmcq': 0.042, 'finance_sent':0.440, 'medqa': 0.019, 'lawqa': 0.06} #bleu\n",
    "it =  {'ni': 0.016, 'alpaca': 0.064, 'medmcq': 0.002, 'finance_sent':0.130, 'medqa': 0.005, 'lawqa': 0.018} #bleu gpt2ap\n",
    "\n",
    "# it =  {'ni': 0.816, 'alpaca': 0.883, 'medmcq': 0.805, 'finance_sent':0.821, 'medqa': 0.830, 'lawqa': 0.831} #bert gpt2ap\n",
    "\n",
    "datasets = zero_shot['dataset'].unique()\n",
    "it_scores = pd.Series(it).reindex(datasets)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "width = 0.2\n",
    "x = range(len(datasets))\n",
    "\n",
    "ax.bar(x, zero_shot.groupby('dataset')[var].mean().reindex(datasets), width, label='Zero Shot')\n",
    "\n",
    "\n",
    "ax.bar([i + width for i in x], one_shot.groupby('dataset')[var].mean().reindex(datasets), width, label='One Shot')\n",
    "\n",
    "ax.bar([i + 2*width for i in x], two_shot.groupby('dataset')[var].mean().reindex(datasets), width, label='Two Shot')\n",
    "\n",
    "ax.bar([i + 3*width for i in x], it_scores, width, label='IT')\n",
    "\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('BLEU')\n",
    "ax.set_title('GPT-2 ICL vs GPT-2 IT (AP)')\n",
    "ax.set_xticks([i + 1.5*width for i in x])\n",
    "ax.set_xticklabels(datasets)\n",
    "\n",
    "category_labels = ['NI', 'AP', 'MEDMCQ', 'ECSA', 'MEDQA', 'LAWQA'] \n",
    "ax.set_xticklabels(category_labels)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.xticks(rotation=45) \n",
    "plt.tight_layout() \n",
    "plt.savefig('gpt2_small_ap_bleu.png') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
