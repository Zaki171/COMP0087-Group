{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICL - GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk, load_metric\n",
    "import json\n",
    "import bert_score\n",
    "import evaluate\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk, load_metric\n",
    "import bert_score\n",
    "import evaluate\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def sent_similarity(sent1, sent2):\n",
    "    sentences = [sent1, sent2]\n",
    "    embeddings = sent_model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix[0][1]\n",
    "\n",
    "\n",
    "def format_examples(ds, ds_name='ni'):\n",
    "    prompts = []\n",
    "    if ds_name == 'ni':\n",
    "        for example in ds:\n",
    "            # prompt = f\"### Question: {example['input']} \\n ###Targets: {example['output']}\"\n",
    "            prompt = f\"### Task: {example['definition']}\\n ### Inputs: {example['inputs']}\\n ### Targets: {example['targets']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'medmcq':\n",
    "         for example in ds:\n",
    "            prompt = f\"### Task: {example['instruction']}\\n ### Question: {example['input']}\\n ### Targets: {example['output']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'finance_sent':\n",
    "        for example in ds:\n",
    "            prompt = f\"### Text: {example['text']}\\n ### Targets: {example['label']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'medqa':\n",
    "        for example in ds:\n",
    "            prompts.append(example['text'])\n",
    "    elif ds_name == 'lawqa':\n",
    "        for example in ds:\n",
    "            prompt = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
    "            prompts.append(prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def select_characters_before_target(string, target_phrase=\"\\n ### Targets:\"): #this is a function to remove the actual target values from the train example so that the matching can be improved\n",
    "    target_index = string.find(target_phrase)\n",
    "    if target_index != -1:  # If the phrase is found\n",
    "        return string[:target_index] + target_phrase\n",
    "    else:\n",
    "        return string \n",
    "    \n",
    "def extract_response_content(string, target_phrase):\n",
    "    response_index = string.find(target_phrase)\n",
    "    return string[response_index + len(target_phrase):].strip()\n",
    "\n",
    "def group_examples_random(ds, n): #this is where we group examples into a larger prompt\n",
    "    random.seed()\n",
    "    samples = random.sample(ds, n)\n",
    "    new_prompt = \"\"\n",
    "    for i in range(n):\n",
    "        new_prompt += samples[i]\n",
    "        new_prompt += \"\\n\"\n",
    "    return new_prompt\n",
    "\n",
    "def group_by_similarity(prompt, ds, n_egs, m_choices, target_phrase=\"\\n ### Targets:\"):\n",
    "    choices = random.sample(ds, m_choices)\n",
    "    cos_sim_dict = {}\n",
    "    for c in choices:\n",
    "        cos_sim_dict[c] = sent_similarity(prompt, select_characters_before_target(c, target_phrase))\n",
    "\n",
    "    sorted_cos_sim = sorted(cos_sim_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_egs = \"\"\n",
    "    for item in sorted_cos_sim[:n_egs]:\n",
    "        top_egs += item[0]\n",
    "        top_egs += \"\\n\"\n",
    "\n",
    "    # top_egs = \"\".join([item[0] for item in sorted_cos_sim[:n_egs]])\n",
    "    return top_egs\n",
    "\n",
    "def count_tokens(tokenizer, prompt):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    return len(input_ids)\n",
    "\n",
    "\n",
    "def evaluate_example(model, tokenizer, prompt, model_name, max_tokens):\n",
    "    if model_name == 'gpt2':\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        if len(tokenized_prompt['input_ids'][0]) > MAX_LENGTH: #currently just checking if random prompt is too big or not\n",
    "            return None \n",
    "        outputs =model.generate(**tokenized_prompt, pad_token_id=tokenizer.eos_token_id, max_length=1024)\n",
    "        decoded_output = tokenizer.decode(outputs[0][len(tokenized_prompt['input_ids'][0]):], skip_special_tokens=True)\n",
    "        return decoded_output\n",
    "    elif model_name == 'mistral':\n",
    "        # num_tokens = count_tokens(tokenizer, prompt)\n",
    "        # print(\"Num tokens in prompt: \", num_tokens)\n",
    "        # if num_tokens > 3400:\n",
    "        #     return None\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        print(\"Max number of tokens: \", max_tokens)\n",
    "        outputs =model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, do_sample=True, max_new_tokens=max_tokens)\n",
    "        decoded_output = tokenizer.decode(outputs[0][len(model_inputs['input_ids'][0]):], skip_special_tokens=True) #only get output\n",
    "        return decoded_output\n",
    "  \n",
    "\n",
    "def evaluate_icl(train_dataset, test_dataset, model, tokenizer, num_egs, model_name, ds_name='ni', method='similarity', max_tokens_dict=None):\n",
    "    reals = []\n",
    "    preds = []\n",
    "    counter = 0\n",
    "    for example in test_dataset:\n",
    "        # prompt = group_examples(train_dataset, num_egs) + f\"### Question: {example['input']} \\n ###Targets:\"\n",
    "        target_phrase = \"\\n ### Targets:\"\n",
    "        if ds_name == 'ni':\n",
    "            curr_prompt = f\"### Task: {example['definition']}\\n ### Inputs: {example['inputs']}\\n ### Targets:\"\n",
    "            real = f\"{example['targets']}\"\n",
    "            max_tokens = max_tokens_dict[real] + 100\n",
    "        elif ds_name == 'medmcq':\n",
    "            curr_prompt = f\"### Task: {example['instruction']}\\n ### Question: {example['input']}\\n ### Targets:\"\n",
    "            real = f\"{example['output']}\"\n",
    "            tokens = tokenizer(real, return_tensors='pt').to(device)\n",
    "            max_tokens = len(tokens['input_ids'][0]) +  100\n",
    "        elif ds_name == 'finance_sent':\n",
    "            curr_prompt = f\"### Text: {example['text']}\\n ### Targets:\"\n",
    "            real = f\"{example['label']}\"\n",
    "            tokens = tokenizer(real, return_tensors='pt').to(device)\n",
    "            max_tokens = len(tokens['input_ids'][0])\n",
    "        elif ds_name == 'medqa':\n",
    "            curr_prompt = select_characters_before_target(example['text'], \"### Response:\")\n",
    "            real = extract_response_content(example['text'], \"### Response:\")\n",
    "            max_tokens = max_tokens_dict[real] + 100\n",
    "        elif ds_name == 'lawqa':\n",
    "            curr_prompt = f\"### Question: {example['question']}\\n ### Answer:\"\n",
    "            real = example['answer']\n",
    "            max_tokens = max_tokens_dict[real] + 100\n",
    "            target_phrase = \"### Answer:\"\n",
    "\n",
    "        # tokens = tokenizer(real, return_tensors='pt').to(device)\n",
    "        # max_tokens = len(tokens['input_ids'][0])\n",
    "    \n",
    "        if method == 'similarity':\n",
    "            icl_prompt = group_by_similarity(curr_prompt,train_dataset, num_egs, 100, target_phrase) + curr_prompt\n",
    "        elif method == 'random':\n",
    "            icl_prompt = group_examples_random(train_dataset, num_egs) + curr_prompt\n",
    "\n",
    "        # print(\"MAX TOKENS:\\n\", max_tokens)\n",
    "        # print(\"\\n ICL Prompt: \",icl_prompt)\n",
    "        print(\"ICL prompt complete\")\n",
    "        pred = evaluate_example(model, tokenizer, icl_prompt, model_name, max_tokens)\n",
    "        print(\"Prediction complete\")\n",
    "\n",
    "        if counter % 50 == 0:\n",
    "            print(\"PROMPT:\\n\", icl_prompt)\n",
    "            print(\"REAL ANSWER:\\n\", real)\n",
    "            print(\"PREDICTION:\\n\", pred)\n",
    "        if pred:\n",
    "            reals.append(real.lower())\n",
    "            preds.append(pred.lower())\n",
    "        counter+=1\n",
    "\n",
    "    return reals, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\u001b[0;32m      2\u001b[0m tokenizer_mist_8\u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model_mist_8 \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-v0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    564\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3307\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3298\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3300\u001b[0m     (\n\u001b[0;32m   3301\u001b[0m         model,\n\u001b[0;32m   3302\u001b[0m         missing_keys,\n\u001b[0;32m   3303\u001b[0m         unexpected_keys,\n\u001b[0;32m   3304\u001b[0m         mismatched_keys,\n\u001b[0;32m   3305\u001b[0m         offload_index,\n\u001b[0;32m   3306\u001b[0m         error_msgs,\n\u001b[1;32m-> 3307\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3314\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3315\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3318\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3319\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3325\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[0;32m   3326\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[1;32mc:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3695\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m   3694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_fsdp_enabled_and_dist_rank_0():\n\u001b[1;32m-> 3695\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3700\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3702\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3703\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3706\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3708\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3710\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3711\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[0;32m   3712\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:706\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m    704\u001b[0m             set_module_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;66;03m# For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "tokenizer_mist_8= AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model_mist_8 = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",  load_in_8bit=True, device_map='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.19s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_plain =  GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "# tokenizer_plain = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# print(\"models retrieved\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "load_in_4bit=True,\n",
    "bnb_4bit_use_double_quant=True,\n",
    "bnb_4bit_quant_type=\"nf4\",\n",
    "bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model_mist = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer_mist = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_gpt2=  GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 64, but `max_length` is set to 5. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n"
     ]
    }
   ],
   "source": [
    "def evaluate_example2(model, tokenizer, prompt):\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    # print(prompt)\n",
    "    # if len(tokenized_prompt['input_ids'][0]) > MAX_LENGTH: #currently just checking if random prompt is too big or not\n",
    "    #     return None \n",
    "    outputs =model.generate(**model_inputs, pad_token_id= tokenizer.eos_token_id, do_sample=False, max_new_tokens = 5)\n",
    "    decoded_output = tokenizer.decode(outputs[0][len(model_inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    # print(\"prediction: \",decoded_output)/\n",
    "    return decoded_output\n",
    "\n",
    "\n",
    "prompt = \"\"\" \"featuring an oscar-worthy performance => positive\\n\"\n",
    "    \"completely messed up => negative\\n\"\n",
    "    \"masterpiece => positive\\n\"\n",
    "    \"the action is stilted => negative\\n\"\n",
    "    \"by far the worst movie of the year =>\" \"\"\"\n",
    "pred = evaluate_example2(model_mist_8, tokenizer_mist_8, prompt) \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Natural Instructions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set:  100\n",
      "Length of train set 575481\n",
      "{'No.': 3, 'Yes.': 3}\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.\n",
      " ### Inputs: Sentence: Jerry goes out to the pier and casts his favorite bait : cheese . \n",
      "Question: How much time did Jerry spend at the pier?\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " No.\n",
      "PREDICTION:\n",
      " yes\n",
      "\n",
      "### Outputs:\n",
      "#### Answer\n",
      "\n",
      "yes\n",
      "\n",
      "#### Rationale\n",
      "\n",
      "Jerry's fishing trip is an explicit mention that answers, given the amount of context information, what we can only call a \"fishing trip\".\n",
      "We consider a sentence explicit if it explicitly spans multiple lexical items (or words). We consider an answer implicit if it is not the answer in the sentence verbatim. In this case answer could be: Goes out to the pier,\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.\n",
      " ### Inputs: Sentence: Delighted , Preetam goes in search of her watch and brings it back . \n",
      "Question: How often does Preetam lose her things and gets them back?\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " No.\n",
      "PREDICTION:\n",
      " \n",
      "    Yes, many times\n",
      "    Yes, once\n",
      "    No, we don't have enough information to decide\n",
      "    No, she does not lose them a lot\n",
      "### Results (Per-token)\n",
      "    ```\n",
      "    Sentence: Delighted , Preetam goes in search of her watch and brings it back\n",
      "    Question: How often does Preetam lose her things and gets them back?\n",
      "\n",
      "    Answer 1: Yes, many times\n",
      "    Token in sentence\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: You will be given a sentence. Check whether the sentence is grammatically correct and is meaningful. If the sentence is grammatically correct, then answer with '1', otherwise answer with '0'.\n",
      " ### Inputs: Bill was bitten the dog.\n",
      " ### Targets: 0\n",
      "### Task: The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.\n",
      " ### Inputs: Sentence: Jerry goes out to the pier and casts his favorite bait : cheese . \n",
      "Question: How much time did Jerry spend at the pier?\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " No.\n",
      "PREDICTION:\n",
      " no\n",
      "### Task: You will be given a sentence in the first field. You will be given a question after that. Answer if the answer is explicitly (not implicitly) present in the sentence, with '1'. Otherwise, answer with '0'. If more than one answer is present in the sentence, then answer with '0' for the question.\n",
      " ### Inputs: He is 23 years old now.\n",
      " Question: How old can you guess he will be after three years? \n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: In this task, you're given a context, a question, and three options. Your task is to find the correct answer to the question using the given context and options. Also, you may need to use commonsense reasoning about social situations to answer the questions. Classify your answers into 'A', 'B', and 'C'.\n",
      " ### Inputs: Context: Lee closed the door behind Sydney so that they could have a private conversation about Lee asking for Sydney's daughter's hand in marriage. \n",
      " Question: How would Lee feel afterwards? \n",
      " Options: (A) nervous (B) marry Sydney's daughter (C) a future groom\n",
      " ### Targets: A\n",
      "### Task: The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.\n",
      " ### Inputs: Sentence: Delighted , Preetam goes in search of her watch and brings it back . \n",
      "Question: How often does Preetam lose her things and gets them back?\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " No.\n",
      "PREDICTION:\n",
      " No\n",
      " ### Task: This task is to find the answer of either 'True' or 'False' for the given question, using commonsense reasoning. Also, you may need to use the given contextual information to solve this task. The question can be about either (or both) the entity or the event in the given sentence.\n",
      " ### Inputs: Context: The 30-day trial is only valid for one user , \n",
      " Question: Can the 30-day free trial\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: Given a statement, generate a question such that the answer is contained in that statement.\n",
      " ### Inputs: nerve extensions receive electrical impulses\n",
      " ### Targets: What do nerve extensions receive?\n",
      "### Task: In this task, you are given a sentence and question which can be answered using the sentence. Your task is to answer the question using the information from the sentence. The answer to the question is unique and it is a continuous text span from the sentence.\n",
      " ### Inputs: Sentence: He also began making super-8 films beginning in junior high , and showed these films to the scholarship committee of Brigham Young University in 1981 , earning a full scholarship in ` Theatre and Cinematic Arts ' after receiving a Sundance Institute ` Most Promising Filmmaker ' award for his film `` Night Meeting '' . \n",
      " Question: who   made something?\n",
      " ### Targets: He\n",
      "### Task: The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.\n",
      " ### Inputs: Sentence: Jerry goes out to the pier and casts his favorite bait : cheese . \n",
      "Question: How much time did Jerry spend at the pier?\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " No.\n",
      "PREDICTION:\n",
      " no\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: In this task, you are given two phrases: Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether, as a result of the Head, PersonX wants what is mentioned in the Tail or not. In this task, wanting is a postcondition desire on the part of PersonX, respectively. As a result of PersonX giving PersonY gifts, PersonX may also desire to hug PersonY. Classify your answers into \"Yes\" and \"No\". The phrase may also contain \"___\", a placeholder that can be an object, a person, and/or an action.\n",
      " ### Inputs: Head: PersonX arrives to the restaurant<sep>Tail: To spend time with X\n",
      " ### Targets: No\n",
      "### Task: In this task, you're given a context, a question, three options, and an answer. Your task is to classify whether the given answer is correct or not by providing 'Yes' or 'No', based on the context with commonsense reasoning about social situations.\n",
      " ### Inputs: Context: Cameron put their tree outside and watered it every day of the year. \n",
      " Question: What will Others want to do next? \n",
      " Options: (A) build a raft (B) watch the tree (C) go to a new town \n",
      " Answer: B\n",
      " ### Targets: Yes\n",
      "### Task: The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.\n",
      " ### Inputs: Sentence: Delighted , Preetam goes in search of her watch and brings it back . \n",
      "Question: How often does Preetam lose her things and gets them back?\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " No.\n",
      "PREDICTION:\n",
      " Yes\n",
      "\n",
      " ### Task: In this task, you are required to predict whether the event in the subject event is an intentional act/action in the context of the given subject. For instance, you are watching a movie, and the character says \"I am hungry\", so it is not an action. However, when the character says \"I will go buy food\", it's an action. This is a different task as you have to identify whether an event is an action or not.\n",
      "### Input\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_mist\n",
    "model = model_mist\n",
    "\n",
    "data = load_from_disk('data/1000_per_task')\n",
    "\n",
    "# data = filter_icl(data, max_num_egs, tokenizer_plain)\n",
    "\n",
    "max_num_egs =  3   #natural instructions are just too big\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "# train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "# train_dataset = train_test_split['train']\n",
    "# test_dataset = train_test_split['test']\n",
    "\n",
    "train_dataset = data['train']\n",
    "test_dataset = data['test'].select(range(100))\n",
    "\n",
    "# def filter_example(example):\n",
    "#     return count_tokens(tokenizer, example['targets']) <= 300\n",
    "\n",
    "# train_dataset = train_dataset.filter(filter_example)\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset)\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "max_token_dict = {}\n",
    "for eg in test_dataset:\n",
    "    max_token_dict[eg['targets']] = count_tokens(tokenizer, eg['targets'])\n",
    "print(max_token_dict)\n",
    "\n",
    "icl_method = 'similarity'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'ni'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "for i in range(max_num_egs):\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model, tokenizer, i, model_name=model_name, ds_name=ds_name, method=icl_method, max_tokens_dict=max_token_dict)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order= order) #set order to mean of real values\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations':i, 'bert_score' : float(average_F1), 'bleu_score': bleu_score['bleu']})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(range(max_num_egs), bert_scores) \n",
    "# plt.xlabel('Number of examples') \n",
    "# plt.ylabel('BERT F1 Score') \n",
    "# plt.title('BERT F1 Score vs Number of Examples') \n",
    "# plt.xticks(range(max_num_egs))\n",
    "# plt.savefig('BERT_scores_icl_ni.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(int(sum(len(s) for s in refs)/len(refs)))\n",
    "# test = bleu.compute(predictions=['No', 'Yes', 'Yes', 'yes', 'Yes', 'yes', 'yes', 'No', 'yes', 'No'],\n",
    "#                     references=['No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.'], max_order=int(sum(len(s) for s in refs)/len(refs)))\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Medical MCQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "max_num_egs = 5\n",
    "\n",
    "data = load_dataset('medalpaca/medical_meadow_medqa')['train']\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='medmcq')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'medmcq'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_mist, tokenizer_mist, i, model_name=model_name, ds_name=ds_name, method=icl_method)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(\"order: \", order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    "    for r,p in zip(reals, preds):\n",
    "        if len(p.strip()) != 0:\n",
    "            if r.strip()[0] == p.strip()[0]:\n",
    "                accuracy+=1\n",
    "    accuracy = accuracy/len(preds)\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu'], 'accuracy':accuracy})\n",
    "\n",
    "# results_df = pd.DataFrame(results_data)\n",
    "# results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(range(max_num_egs), bert_scores) \n",
    "# plt.xlabel('Number of examples') \n",
    "# plt.ylabel('BERT F1 Score')\n",
    "# plt.title('BERT F1 Score vs Number of Examples') \n",
    "# plt.xticks(range(max_num_egs))\n",
    "# plt.savefig('BERT_scores_icl_medqa.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Transcript Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_num_egs = 5\n",
    "\n",
    "data = load_dataset('jlh-ibm/earnings_call', 'transcript-sentiment')['train']\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='finance_sent')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "icl_method = 'similarity'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'finance_sent'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_mist, tokenizer_mist, i, model_name=model_name, ds_name=ds_name, method=icl_method)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(\"order: \", order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    "    for r,p in zip(reals, preds):\n",
    "        if len(p.strip()) != 0:\n",
    "            if r.strip()[0] == p.strip()[0]:\n",
    "                accuracy+=1\n",
    "    accuracy = accuracy/len(preds)\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu'], 'accuracy':accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Medicine QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15549\n",
      "Length of test set:  100\n",
      "Length of train set 12439\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    Is anencephaly inherited ?\n",
      "    ### Response:\n",
      "REAL ANSWER:\n",
      " Most cases of anencephaly are sporadic, which means they occur in people with no history of the disorder in their family. A small percentage of cases have been reported to run in families; however, the condition does not have a clear pattern of inheritance.\n",
      "PREDICTION:\n",
      " \n",
      "    Anencephaly is not inherited. Anencephaly is a result of random, uncontrollable genetic mutations that occur after the fertilized egg begins to expand and divide into two cells , then four , then eight , and so on .\n",
      "    ### Instruction:\n",
      "    Is there a genetic component to anencephaly? Does a family history of anencephaly increase the risk of having a child born with anencephaly?\n",
      "    ### Response:\n",
      "    Anencephaly is considered to be a non-inheritable birth defect with a high recurrence rate . Therefore there is a great chance of it happening again . However a family history of anencephaly does not usually indicate the risk of a child being born with\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    What is (are) Parasites - Fascioliasis (Fasciola Infection) ?\n",
      "    ### Response:\n",
      "REAL ANSWER:\n",
      " Fascioliasis is an infectious disease caused by Fasciola parasites, which are flat worms referred to as liver flukes. The adult (mature) flukes are found in the bile ducts and liver of infected people and animals, such as sheep and cattle. In general, fascioliasis is more common in livestock and other animals than in people.\n",
      "PREDICTION:\n",
      " \n",
      "    A parasite is a species that lives in or with a host organism or species. For its nutrition the parasite depends on that from the host or on the products of its metabolism. However, parasites do not produce their own food by themselves.\n",
      "    A parasite exists by either symbiosis or direct harm to the host. Many parasites harm the host, where they may reduce the efficiency of metabolism (eg by eating it from the inside).\n",
      "    Parasites come in many forms. Those that live within the cells of a multi-cellular host are called intracellular parasites (e.g., viruses, bacteria). One of the best known parasites is the plasmodium (the carrier of malaria). Other parasites live on the surface of the cell (e.g., certain viruses). These are called non-cell\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order:  1\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    Is Fryns syndrome inherited ?\n",
      "    ### Response:\n",
      "    How is Fryns syndrome inherited? Although the exact cause of Fryns syndrome is not currently known (and no disease-causing gene has yet been identified), it is thought to be genetic because it tends to \"run in families\" and has features common to other genetic disorders.\n",
      "    \n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    Is anencephaly inherited ?\n",
      "    ### Response:\n",
      "REAL ANSWER:\n",
      " Most cases of anencephaly are sporadic, which means they occur in people with no history of the disorder in their family. A small percentage of cases have been reported to run in families; however, the condition does not have a clear pattern of inheritance.\n",
      "PREDICTION:\n",
      " \n",
      "    Anencephaly is not inherited or genetic. Even though anencephaly has been linked to other genetic conditions (such as trisomy 18), most women who had a baby affected by anencephaly have babies who are not affected. Studies have shown, however, that some of the risk factors for anencephaly can be inherited\n",
      "    \n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    Why is Bardet-Biedl syndrome considered a form of polydactyly?\n",
      "    ### Response:\n",
      "    \"Bardet-Biedl syndrome (BBS) is a genetic, autosomal, inherited disorder characterized by the combined presence of six abnormal\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    What is (are) Problems with Smell ?\n",
      "    ### Response:\n",
      "    You can help your doctor make a diagnosis by writing down important information about your problem beforehand and giving the information to your doctor during your visit. Write down answers to the following questions. - When did I first become aware of the \n",
      "    \n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    What is (are) Parasites - Fascioliasis (Fasciola Infection) ?\n",
      "    ### Response:\n",
      "REAL ANSWER:\n",
      " Fascioliasis is an infectious disease caused by Fasciola parasites, which are flat worms referred to as liver flukes. The adult (mature) flukes are found in the bile ducts and liver of infected people and animals, such as sheep and cattle. In general, fascioliasis is more common in livestock and other animals than in people.\n",
      "PREDICTION:\n",
      " \n",
      "    In the United States, the life cycle of the parasite Fasciola hepatica occurs in cattle and other domestic ruminants. Cattle serve as definitive hosts and are infected by larvae through grazing on pasture containing the larval-infested intermediate hosts (snails) or from drinking water contaminated by the larvae. Fasciola hepatica\n",
      "\n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    What is (are) Parasites - Parabasalids (Trichomonads) ?\n",
      "    ### Response:\n",
      "    Parabasalids are a large group of protozoa of variable size and shape, and are distributed widely in nature. They consist of seven genera, including Trichomonas spp. Their size and shape may differ considerably within and between\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order:  1\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    Is Rett syndrome inherited ?\n",
      "    ### Response:\n",
      "    In more than 99 percent of people with Rett syndrome, there is no history of the disorder in their family. Many of these cases result from new mutations in the MECP2 gene.  A few families with more than one affected family member have been described. These cases helped researchers determine that classic Rett syndrome and variants caused by MECP2 gene mutations have an X-linked dominant pattern of inheritance.\n",
      "    \n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    Is Chorea-acanthocytosis inherited ?\n",
      "    ### Response:\n",
      "    How do people inherit chorea-acanthocytosis? Chorea-acanthocytosis is inherited in an autosomal recessive pattern, which means both copies of the gene in each cell have mutations. The parents of an individual with an autosomal recessive condition each carry one copy of the mutated gene, but they typically do not show signs and symptoms of the condition.\n",
      "    \n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    Is anencephaly inherited ?\n",
      "    ### Response:\n",
      "REAL ANSWER:\n",
      " Most cases of anencephaly are sporadic, which means they occur in people with no history of the disorder in their family. A small percentage of cases have been reported to run in families; however, the condition does not have a clear pattern of inheritance.\n",
      "PREDICTION:\n",
      " \n",
      "    Anencephaly doesn't happen because of gene problems in the unborn baby. Instead, it happens because of a combination of genetic and environmental factors that occur within the first 3 to 4 weeks of pregnancy.\n",
      "    \n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    Is Trousseau syndrome inherited ?\n",
      "    ### Response:\n",
      "    Not enough cases are known about Trousseau syndrome and it is not inherited.\n",
      "    \n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    Is Mafalys syndrome inherited ?\n",
      "    ### Response:\n",
      "    People inherit Mafalys syndrome through an X-linked recessive genetic pattern. What\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    What is (are) Plague ?\n",
      "    ### Response:\n",
      "    Plague is an infection caused by the bacterium Yersinia pestis. The bacteria are found mainly in rats and in the fleas that feed on them. People and other animals can get plague from rat or flea bites. In the past, plague destroyed entire civilizations. Today plague is uncommon, due to better living conditions and antibiotics.\n",
      "    \n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    What are the symptoms of Aplasia cutis congenita ?\n",
      "    ### Response:\n",
      "    What are the signs and symptoms of Aplasia cutis congenita? The Human Phenotype Ontology provides the following list of signs and symptoms for Aplasia cutis congenita. If the information is available, the table below includes how often the symptom is seen in people with this condition.\n",
      "    \n",
      "Below is an instruction from Human. Write a response.\n",
      "    ### Instruction:\n",
      "    What is (are) Parasites - Fascioliasis (Fasciola Infection) ?\n",
      "    ### Response:\n",
      "REAL ANSWER:\n",
      " Fascioliasis is an infectious disease caused by Fasciola parasites, which are flat worms referred to as liver flukes. The adult (mature) flukes are found in the bile ducts and liver of infected people and animals, such as sheep and cattle. In general, fascioliasis is more common in livestock and other animals than in people.\n",
      "PREDICTION:\n",
      " \n",
      "    What is fascioliasis? Fascioliasis is an infection caused by parasites in the liver -- generally from Fasciola hepatica. It only affects certain areas of the world with suitable climate (temperature and humidity levels) and water sources. The parasite must go through a very specific cycle to spread (requires a water source) and a specific geographical location (requires enough temperature/humidity for the parasite to survive). Many things can cause your body to do what it does, so a diagnosis of a specific disease is not always that simple. Since it is not possible to know for sure what is going on with your body, and since all sorts of conditions can cause a variety of things (including abnormal bloodwork), for all we know it could have nothing at all to do with fascioliasis. It would be wrong to assume you have\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_num_egs = 3\n",
    "\n",
    "data = load_dataset('Laurent1/MedQuad-MedicalQnADataset_128tokens_max')['train']\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "def filter_example(example):\n",
    "    return count_tokens(tokenizer_mist, extract_response_content(example['text'], \"### Response:\")) <= 300\n",
    "\n",
    "data = data.filter(filter_example)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "\n",
    "max_token_dict = {}\n",
    "for example in test_dataset:\n",
    "    real = extract_response_content(example['text'], \"### Response:\")\n",
    "    max_token_dict[real] = count_tokens(tokenizer_mist, real)\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='medqa')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "# avg_tokens = 0\n",
    "# for eg in train_list:\n",
    "#     avg_tokens+= count_tokens(tokenizer_mist, eg)\n",
    "# avg_tokens  = avg_tokens/len(train_list)\n",
    "# print(\"AVG TOKENS: \",avg_tokens)\n",
    "#avg_tokens = \n",
    "\n",
    "\n",
    "icl_method = 'similarity'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'medqa'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_mist, tokenizer_mist, i, model_name=model_name, ds_name=ds_name, method=icl_method, max_tokens_dict=max_token_dict)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(\"order: \", order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    " \n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu']})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "\n",
    "Mistral 8B?\n",
    "Law dataset\n",
    "Figure out max tokens crap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102\n",
      "Length of test set:  100\n",
      "Length of train set 881\n",
      "Token Dict complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  159\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Question: Q: What should I do? My account is frozen cause I was given fraud checks and bank took my money when they decline the check. So I was given two checks for a side job I deposit it to my account and they decline next day I was called that the check’s were fraud but they bank never gave me the money. I gave them the emails and address of where I gotten them and they froze my account. When I login into my account I had 123.50 in the checking account I checked the next day it was 232 which was weird then I saw it was taken . I don’t know how if it’s frozen \n",
      " ### Answer:\n",
      "REAL ANSWER:\n",
      " A:If you never got the money and the checks were declined, the bank must suspect you for fraud. There is something not right but if the whole matter is over $110 there is little a lawyer can do. When the courts reopen, make a small claims suit.\n",
      "PREDICTION:\n",
      " \n",
      "\n",
      "1. There is no way for us to definitively say if your account is legitimately frozen for no reason - however, if you do contact your banking institution (if you have one) and find out why it's closed, please do let us know, even if we do not solve your issue/problem, your experience will still be helpful for those that need help from a similar issue.\n",
      "ICL prompt complete\n",
      "Max number of tokens:  190\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  137\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  112\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  173\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  178\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  184\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  178\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  194\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  178\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  139\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  176\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  169\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  113\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  167\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  180\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  157\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  117\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  183\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  165\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  191\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  185\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  197\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  151\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  143\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  174\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  193\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  171\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  180\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  176\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  128\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  126\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Question: Q: Hello! Thank you that you have this option. I used the inscription \"PUBG MOBILE\" on the photo. And my account was ban!. I don't know can they delete my account only because I use these 2 words \n",
      " ### Answer:\n",
      "REAL ANSWER:\n",
      " A:You can contest the ban, ask for explanation. I am not familiar with the meaning, but if it is related to a registered brand you may have infringed on somebody's rights. Consult with an attorney.\n",
      "PREDICTION:\n",
      "  There is something in the TOS, that players should not put the name of another game in their nicknames.\n",
      "\n",
      " ### Question: i was banned for 1 year for no reason. my name is a popular song name and that is not even related to something bad, what can i do with that?\n",
      " ### Answer:  Hello. I'd recommend you to contact support. It's a pity for you but as for now account can not be changed.\n",
      "\n",
      " ### Question: Hello, I don’t know what happened, why was my account banned for 1 Year?. I am so sad now,  please forgive me!\n",
      " ### Answer:  It's not known why your account has\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  139\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  135\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  155\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  189\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  138\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  142\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  180\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  159\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  153\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  197\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  145\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  193\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  137\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  184\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  184\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  135\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  144\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  199\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  130\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  195\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  155\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  162\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  181\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  166\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  165\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  162\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  197\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  137\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  189\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  112\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  164\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  161\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt complete\n",
      "Max number of tokens:  159\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Question: Q: I recently financed a ‘13 Dodge Avenger from a used car dealership. It failed 2 emissions inspections.. The car has been at the mechanic longer than I have been able to use it and I’m making payments on it for 2 months now without being able to use the vehicle. The dealership refuses to void the contract and insists of wasting more time trying to fix it or selling me a different car with a higher monthly payment (which I would have initially financed if I could afford it). The car has had other several repairs done since first date of purchase such as caliper changers, brake pad repairs and tire replacements and still has other issues that were ignored by the mechanic such as a non functional horn and head light is out. I have reported a complaint to consumer fraud department and I’m waiting to hear back, but the car should have never been sold to me in this condition and I just want to have them take it back and void the contract. \n",
      " ### Answer: A:This question was previously asked and answered. Good luck.\n",
      "### Question: Q: What should I do? My account is frozen cause I was given fraud checks and bank took my money when they decline the check. So I was given two checks for a side job I deposit it to my account and they decline next day I was called that the check’s were fraud but they bank never gave me the money. I gave them the emails and address of where I gotten them and they froze my account. When I login into my account I had 123.50 in the checking account I checked the next day it was 232 which was weird then I saw it was taken . I don’t know how if it’s frozen \n",
      " ### Answer:\n",
      "REAL ANSWER:\n",
      " A:If you never got the money and the checks were declined, the bank must suspect you for fraud. There is something not right but if the whole matter is over $110 there is little a lawyer can do. When the courts reopen, make a small claims suit.\n",
      "PREDICTION:\n",
      " A:You need to contact the authorities and report it. Let them know that all the accounts including banking are being messed up.\n",
      "ICL prompt complete\n",
      "Max number of tokens:  190\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  137\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  112\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  173\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  178\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  184\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  178\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  194\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  178\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  139\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  176\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  169\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  113\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  167\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  180\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  157\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  117\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  183\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  165\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  191\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  185\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  197\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  151\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  143\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  174\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  193\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  171\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  180\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  176\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  128\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  126\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Question: Q: Being only insured driver ever on policy, I had full coverage with permissive drivers. I had two comprehensive claims. I had my car fixed, crappily, another story. After receiving my car , I believe another premium was paid, then I was dropped. There's SEVERAL ISSUES, THE SAID I WAS DRIVER OF COBALT FROM ARKANSAS AND ADDED FULL COVERAGE ON A MOTOR, MY SONS, I HAVE NEVER HAD A MOTORCYCLE OR AN ENDORSEMENT! I HAVE PAPERWORK IN REGUARDS TO THIS , PLUS SOME! \n",
      " ### Answer: A:An Alabama or Arkansas attorney could advise best, but your post remains open for a week. Until you are able to consult with a local attorney, you could contact the carrier and request their reason for dropping you. The short answer is that insurance carriers can drop policyholders based on their risk, but there can also be insurance or consumer laws that apply. You could check with your state's Department of Insurance or Department of Consumer Affairs for guidance. Good luck\n",
      "### Question: Q: Hello! Thank you that you have this option. I used the inscription \"PUBG MOBILE\" on the photo. And my account was ban!. I don't know can they delete my account only because I use these 2 words \n",
      " ### Answer:\n",
      "REAL ANSWER:\n",
      " A:You can contest the ban, ask for explanation. I am not familiar with the meaning, but if it is related to a registered brand you may have infringed on somebody's rights. Consult with an attorney.\n",
      "PREDICTION:\n",
      " A:I will look over my email but it is unlikely I can do anything. You could email their customer assistance for help. Thanks\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  139\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  135\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  155\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  189\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  138\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  142\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  180\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  159\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  153\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  197\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  145\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  193\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  137\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  184\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  184\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  135\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  144\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  199\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  130\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  195\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  155\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  162\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  181\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  166\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  165\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  162\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  197\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  137\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  189\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  112\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  164\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  161\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt complete\n",
      "Max number of tokens:  159\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Question: Q: My nephew wants to build a house on land I own. He said my name has to come off the deeds. Is that true?. I have no problem with him building a house on the land but I want my name to remain on the deeds. \n",
      " ### Answer: A:If you own the land and your nephew wants to build a house on it, it is not necessarily true that your name has to be removed from the deeds. The ownership of the land can be structured in a way that allows your nephew to build the house while still maintaining your ownership rights.\n",
      "### Question: Q: What does GMAC, WVMF Funding, or RECAP have to do with this case?. Does it have something to do with the mortgage crash in 2008 through 2010? \n",
      " ### Answer: A:A South Carolina attorney could best advise, but your post remains open for five weeks. It's possible something inadvertently got left off in uploading your post - a case is not mentioned. Not every question is picked up, but you could try reposting, including the name of the case. Good luck Tim Akpinar\n",
      "### Question: Q: What should I do? My account is frozen cause I was given fraud checks and bank took my money when they decline the check. So I was given two checks for a side job I deposit it to my account and they decline next day I was called that the check’s were fraud but they bank never gave me the money. I gave them the emails and address of where I gotten them and they froze my account. When I login into my account I had 123.50 in the checking account I checked the next day it was 232 which was weird then I saw it was taken . I don’t know how if it’s frozen \n",
      " ### Answer:\n",
      "REAL ANSWER:\n",
      " A:If you never got the money and the checks were declined, the bank must suspect you for fraud. There is something not right but if the whole matter is over $110 there is little a lawyer can do. When the courts reopen, make a small claims suit.\n",
      "PREDICTION:\n",
      " A:This isn't specific to your state, but the way it would work in New York would be that the checks from the bad account were drawn against a legitimate account at that bank. The legitimate account was then frozen because of the deposited checks. You didn't have a right to the money there, so it would be better to see if you can get the money back from the store or person where the checks were provided.\n",
      "### Question: Q: How can we legally prove that I actually own property in which my husband and I jointly purchased and is being legally disputed by my husband? \n",
      " ### Answer: A:To do this, you just need to come to the court with whatever proof you have of your ownership interest in the property. The most common\n",
      "ICL prompt complete\n",
      "Max number of tokens:  190\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  137\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  112\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  173\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  178\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  184\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  178\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  194\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  178\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  139\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  176\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  169\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  113\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  167\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  180\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  157\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  117\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  183\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  165\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  191\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  185\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  197\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  151\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  143\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  177\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  174\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  193\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  171\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  180\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  176\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  128\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  126\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Question: Q: How can I serve my spouse divorce papers if he is homeless? His last known address is in TX, he is currently in MN.. He told me to send the papers to his mother's address in TX, however, he is currently in MN with no place to stay. \n",
      " ### Answer: A:You may qualify for a divorce by publication if his location is truly unknown. However, with a divorce by publication the court is limited on the issues they can decide and often will be prohibited from making determinations on issues such as child support and equitable division if there is no personal service of the defendant.\n",
      "### Question: Q: How do I check to see if my step mother has petitioned to the clerk regarding my Father’s ( her husband) mental capacit?. I live in Canada and my Father is a US Citizen in North Carolina and there is an issue of trust \n",
      " ### Answer: A:In North Carolina, to check if your stepmother has filed a petition regarding your father's mental capacity, you can contact the Clerk of Superior Court's office in the county where your father resides. They manage guardianship and competency proceedings. Remember, guardianship proceedings differ from a Power of Attorney, which might not be public record. For personalized legal advice tailored to your unique circumstances, it's advisable to consult with an attorney.\n",
      "### Question: Q: Hello! Thank you that you have this option. I used the inscription \"PUBG MOBILE\" on the photo. And my account was ban!. I don't know can they delete my account only because I use these 2 words \n",
      " ### Answer:\n",
      "REAL ANSWER:\n",
      " A:You can contest the ban, ask for explanation. I am not familiar with the meaning, but if it is related to a registered brand you may have infringed on somebody's rights. Consult with an attorney.\n",
      "PREDICTION:\n",
      " A:Do you have proof that your account was banned due to the use of these two words? Did you receive a warning or any other form of communication from the platform or company which you played the game on? It would be useful to have any correspondence from them relating to the ban. \n",
      "\n",
      "### Answer: A:Did you write something with those \"two words\". Usually, you need to do something to be banned from the game and not just having those words.\n",
      "### Question: Q: Can an attorney make a claim on an inheritance on behalf of my mother and if so what documents are required.?\n",
      " ### Answer: A:Generally speaking, an attorney cannot make a claim on their client's\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  139\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  135\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  155\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  189\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  138\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  142\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  180\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  159\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  153\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  197\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  145\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  193\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  137\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  184\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  184\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  135\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  144\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  199\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  130\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  195\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  168\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  155\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  162\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  181\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  166\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  165\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  162\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  197\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  147\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  137\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  189\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  112\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  164\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  161\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_num_egs = 3\n",
    "\n",
    "data = load_dataset('dzunggg/legal-qa-v1')['train']\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "def filter_example(example):\n",
    "    return count_tokens(tokenizer_mist, example['answer']) <= 100\n",
    "\n",
    "data = data.filter(filter_example)\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='lawqa')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "max_token_dict = {}\n",
    "for eg in test_dataset:\n",
    "    max_token_dict[eg['answer']] = count_tokens(tokenizer_mist, eg['answer'])\n",
    "\n",
    "# avg_tokens = 0\n",
    "# for eg in train_list:\n",
    "#     avg_tokens+= count_tokens(tokenizer_mist, eg)\n",
    "# avg_tokens  = avg_tokens/len(train_list)\n",
    "# print(\"AVG TOKENS: \",avg_tokens)\n",
    "#avg_tokens = \n",
    "\n",
    "\n",
    "\n",
    "print(\"Token Dict complete\")\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'lawqa'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_mist, tokenizer_mist, i, model_name=model_name, ds_name=ds_name, method=icl_method, max_tokens_dict=max_token_dict)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    " \n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu']})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
