{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICL - GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk, load_metric\n",
    "import json\n",
    "import bert_score\n",
    "import evaluate\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "MAX_LENGTH = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk, load_metric\n",
    "import bert_score\n",
    "import evaluate\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def sent_similarity(sent1, sent2):\n",
    "    sentences = [sent1, sent2]\n",
    "    embeddings = sent_model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix[0][1]\n",
    "\n",
    "\n",
    "def format_examples(ds, ds_name='ni'):\n",
    "    prompts = []\n",
    "    if ds_name == 'ni':\n",
    "        for example in ds:\n",
    "            # prompt = f\"### Question: {example['input']} \\n ###Targets: {example['output']}\"\n",
    "            prompt = f\"### Task: {example['definition']}\\n ### Inputs: {example['inputs']}\\n ### Targets: {example['targets']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'medmcq':\n",
    "         for example in ds:\n",
    "            prompt = f\"### Task: {example['instruction']}\\n ### Question: {example['input']}\\n ### Targets: {example['output']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'finance_sent':\n",
    "        for example in ds:\n",
    "            prompt = f\"### Text: {example['text']}\\n ### Targets: {example['label']}\"\n",
    "            prompts.append(prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def select_characters_before_target(string):\n",
    "    target_phrase = \"### Targets:\"\n",
    "    target_index = string.find(target_phrase)\n",
    "    if target_index != -1:  # If the phrase is found\n",
    "        return string[:target_index] + target_phrase\n",
    "    else:\n",
    "        return string \n",
    "\n",
    "def group_examples_random(ds, n): #this is where we group examples into a larger prompt\n",
    "    random.seed()\n",
    "    samples = random.sample(ds, n)\n",
    "    new_prompt = \"\"\n",
    "    for i in range(n):\n",
    "        new_prompt += samples[i]\n",
    "        new_prompt += \"\\n\"\n",
    "    return new_prompt\n",
    "\n",
    "def group_by_similarity(prompt, ds, n_egs, m_choices):\n",
    "    choices = random.sample(ds, m_choices)\n",
    "    cos_sim_dict = {}\n",
    "    for c in choices:\n",
    "        cos_sim_dict[c] = sent_similarity(prompt, select_characters_before_target(c))\n",
    "\n",
    "    sorted_cos_sim = sorted(cos_sim_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_egs = \"\"\n",
    "    for item in sorted_cos_sim[:n_egs]:\n",
    "        top_egs += item[0]\n",
    "        top_egs += \"\\n\"\n",
    "\n",
    "    top_egs = \"\".join([item[0] for item in sorted_cos_sim[:n_egs]])\n",
    "    return top_egs\n",
    "\n",
    "def count_tokens(tokenizer, prompt):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    return len(input_ids)\n",
    "\n",
    "\n",
    "def evaluate_example(model, tokenizer, prompt, model_name, max_tokens):\n",
    "    if model_name == 'gpt2':\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        if len(tokenized_prompt['input_ids'][0]) > MAX_LENGTH: #currently just checking if random prompt is too big or not\n",
    "            return None \n",
    "        outputs =model.generate(**tokenized_prompt, pad_token_id=tokenizer.eos_token_id, max_length=1024)\n",
    "        decoded_output = tokenizer.decode(outputs[0][len(tokenized_prompt['input_ids'][0]):], skip_special_tokens=True)\n",
    "        return decoded_output\n",
    "    elif model_name == 'mistral':\n",
    "        num_tokens = count_tokens(tokenizer, prompt)\n",
    "        if num_tokens > 3500:\n",
    "            return None\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        outputs =model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, do_sample=True, max_new_tokens=max_tokens)\n",
    "        decoded_output = tokenizer.decode(outputs[0][len(model_inputs['input_ids'][0]):], skip_special_tokens=True) #only get output\n",
    "        return decoded_output\n",
    "  \n",
    "\n",
    "def evaluate_icl(train_dataset, test_dataset, model, tokenizer, num_egs, model_name, ds_name='ni', method='similarity'):\n",
    "    reals = []\n",
    "    preds = []\n",
    "    counter = 0\n",
    "    for example in test_dataset:\n",
    "        # print(example)\n",
    "        # prompt = group_examples(train_dataset, num_egs) + f\"### Question: {example['input']} \\n ###Targets:\"\n",
    "        if ds_name == 'ni':\n",
    "            curr_prompt = f\"### Task: {example['definition']}\\n ### Inputs: {example['inputs']}\\n ### Targets:\"\n",
    "            real = f\"{example['targets']}\"\n",
    "        elif ds_name == 'medqa':\n",
    "            curr_prompt = f\"### Task: {example['instruction']}\\n ### Question: {example['input']}\\n ### Targets:\"\n",
    "            real = f\"{example['output']}\"\n",
    "        elif ds_name == 'finance_sent':\n",
    "            curr_prompt = f\"### Text: {example['text']}\\n ### Targets:\"\n",
    "            real = f\"{example['label']}\"\n",
    "\n",
    "        tokens = tokenizer(real, return_tensors='pt').to(device)\n",
    "        max_tokens = len(tokens['input_ids'][0])\n",
    "    \n",
    "        if method == 'similarity':\n",
    "            icl_prompt = group_by_similarity(curr_prompt,train_dataset, num_egs, 100) + curr_prompt\n",
    "        elif method == 'random':\n",
    "            icl_prompt = group_examples_random(train_dataset, num_egs) + curr_prompt\n",
    "\n",
    "        # print(\"MAX TOKENS:\\n\", max_tokens)\n",
    "\n",
    "        pred = evaluate_example(model, tokenizer, icl_prompt, model_name, max_tokens)\n",
    "\n",
    "        if counter % 50 == 0:\n",
    "            print(\"PROMPT:\\n\", icl_prompt)\n",
    "            print(\"REAL ANSWER:\\n\", real)\n",
    "            print(\"PREDICTION:\\n\", pred)\n",
    "        if pred:\n",
    "            reals.append(real)\n",
    "            preds.append(pred)\n",
    "        counter+=1\n",
    "\n",
    "    return reals, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:38<00:00, 19.19s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_plain =  GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "# tokenizer_plain = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# print(\"models retrieved\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "load_in_4bit=True,\n",
    "bnb_4bit_use_double_quant=True,\n",
    "bnb_4bit_quant_type=\"nf4\",\n",
    "bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model_mist = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer_mist = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_gpt2=  GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 64, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n"
     ]
    }
   ],
   "source": [
    "def evaluate_example2(model, tokenizer, prompt):\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    # print(prompt)\n",
    "    # if len(tokenized_prompt['input_ids'][0]) > MAX_LENGTH: #currently just checking if random prompt is too big or not\n",
    "    #     return None \n",
    "    outputs =model.generate(**model_inputs, pad_token_id= tokenizer.eos_token_id, do_sample=False, max_length = 10)\n",
    "    decoded_output = tokenizer.decode(outputs[0][len(model_inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    # print(\"prediction: \",decoded_output)/\n",
    "    return decoded_output\n",
    "\n",
    "\n",
    "prompt = \"\"\" \"featuring an oscar-worthy performance => positive\\n\"\n",
    "    \"completely messed up => negative\\n\"\n",
    "    \"masterpiece => positive\\n\"\n",
    "    \"the action is stilted => negative\\n\"\n",
    "    \"by far the worst movie of the year =>\" \"\"\"\n",
    "pred = evaluate_example2(model_mist, tokenizer_mist, prompt) \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Natural Instructions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk('data/1000_per_task')\n",
    "\n",
    "# data = filter_icl(data, max_num_egs, tokenizer_plain)\n",
    "\n",
    "max_num_egs =  5   #natural instructions are just too big\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "# train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "# train_dataset = train_test_split['train']\n",
    "# test_dataset = train_test_split['test']\n",
    "\n",
    "train_dataset = data['train']\n",
    "test_dataset = data['test'].select(range(5))\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset)\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'ni'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "for i in range(max_num_egs):\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_mist, tokenizer_mist, i, model_name=model_name, ds_name=ds_name, method=icl_method)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order= order) #set order to mean of real values\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations':i, 'bert_score' : float(average_F1), 'bleu_score': bleu_score['bleu']})\n",
    "\n",
    "# results_df = pd.DataFrame(results_data)\n",
    "# results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(range(max_num_egs), bert_scores) \n",
    "# plt.xlabel('Number of examples') \n",
    "# plt.ylabel('BERT F1 Score') \n",
    "# plt.title('BERT F1 Score vs Number of Examples') \n",
    "# plt.xticks(range(max_num_egs))\n",
    "# plt.savefig('BERT_scores_icl_ni.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.07357588823428847,\n",
       " 'precisions': [0.2],\n",
       " 'brevity_penalty': 0.36787944117144233,\n",
       " 'length_ratio': 0.5,\n",
       " 'translation_length': 10,\n",
       " 'reference_length': 20}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(int(sum(len(s) for s in refs)/len(refs)))\n",
    "# test = bleu.compute(predictions=['No', 'Yes', 'Yes', 'yes', 'Yes', 'yes', 'yes', 'No', 'yes', 'No'],\n",
    "#                     references=['No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.'], max_order=int(sum(len(s) for s in refs)/len(refs)))\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Medical MCQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "max_num_egs = 5\n",
    "\n",
    "data = load_dataset('medalpaca/medical_meadow_medqa')['train']\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='medqa')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'medmcq'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_mist, tokenizer_mist, i, model_name=model_name, ds_name=ds_name, method=icl_method)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(\"order: \", order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    "    for r,p in zip(reals, preds):\n",
    "        if len(p.strip()) != 0:\n",
    "            if r.strip()[0] == p.strip()[0]:\n",
    "                accuracy+=1\n",
    "    accuracy = accuracy/len(preds)\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu'], 'accuracy':accuracy})\n",
    "\n",
    "# results_df = pd.DataFrame(results_data)\n",
    "# results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(range(max_num_egs), bert_scores) \n",
    "# plt.xlabel('Number of examples') \n",
    "# plt.ylabel('BERT F1 Score')\n",
    "# plt.title('BERT F1 Score vs Number of Examples') \n",
    "# plt.xticks(range(max_num_egs))\n",
    "# plt.savefig('BERT_scores_icl_medqa.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_demonstrations</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0.814683</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.020408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911485</td>\n",
       "      <td>0.407569</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0.917994</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.923827</td>\n",
       "      <td>0.432153</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>0.911291</td>\n",
       "      <td>0.406805</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_samples  num_demonstrations  bert_score  bleu_score  accuracy\n",
       "0           98                   0    0.814683    0.110400  0.020408\n",
       "1          100                   1    0.911485    0.407569  0.240000\n",
       "2          100                   2    0.917994    0.432900  0.300000\n",
       "3          100                   3    0.923827    0.432153  0.280000\n",
       "4          100                   4    0.911291    0.406805  0.190000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_data)\n",
    "# results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Transcript Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_num_egs = 5\n",
    "\n",
    "data = load_dataset('jlh-ibm/earnings_call', 'transcript-sentiment')['train']\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='finance_sent')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'finance_sent'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_mist, tokenizer_mist, i, model_name=model_name, ds_name=ds_name, method=icl_method)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(\"order: \", order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    "    for r,p in zip(reals, preds):\n",
    "        if len(p.strip()) != 0:\n",
    "            if r.strip()[0] == p.strip()[0]:\n",
    "                accuracy+=1\n",
    "    accuracy = accuracy/len(preds)\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu'], 'accuracy':accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_demonstrations</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0.727094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.953518</td>\n",
       "      <td>0.396396</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0.964684</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>0.963729</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.530612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "      <td>0.970722</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.597938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_samples  num_demonstrations  bert_score  bleu_score  accuracy\n",
       "0           99                   0    0.727094    0.000000  0.000000\n",
       "1          100                   1    0.953518    0.396396  0.450000\n",
       "2          100                   2    0.964684    0.471698  0.520000\n",
       "3           98                   3    0.963729    0.495050  0.530612\n",
       "4           97                   4    0.970722    0.587629  0.597938"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
