{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICL - GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk, load_metric\n",
    "import bert_score\n",
    "import evaluate\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def sent_similarity(sent1, sent2):\n",
    "    sentences = [sent1, sent2]\n",
    "    embeddings = sent_model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix[0][1]\n",
    "\n",
    "\n",
    "def format_examples(ds, ds_name='ni'):\n",
    "    prompts = []\n",
    "    if ds_name == 'ni':\n",
    "        for example in ds:\n",
    "            # prompt = f\"### Question: {example['input']} \\n ###Targets: {example['output']}\"\n",
    "            prompt = f\"### Task: {example['definition']}\\n ### Inputs: {example['inputs']}\\n ### Targets: {example['targets']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'medmcq':\n",
    "         for example in ds:\n",
    "            prompt = f\"### Task: {example['instruction']}\\n ### Question: {example['input']}\\n ### Targets: {example['output']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'finance_sent':\n",
    "        for example in ds:\n",
    "            prompt = f\"### Text: {example['text']}\\n ### Targets: {example['label']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'medqa':\n",
    "        for example in ds:\n",
    "            prompts.append(example['text'])\n",
    "    elif ds_name == 'lawqa':\n",
    "        for example in ds:\n",
    "            prompt = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'alpaca':\n",
    "        for example in ds:\n",
    "            prompt = f\"### Instruction: {example['instruction']}\\n ### Input: {example['input']}\\n ### Text: {example['text']} \\n ### Output: {example['output']}\"\n",
    "            prompts.append(prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def select_characters_before_target(string, target_phrase=\"\\n ### Targets:\"): #this is a function to remove the actual target values from the train example so that the matching can be improved\n",
    "    target_index = string.find(target_phrase)\n",
    "    if target_index != -1:  # If the phrase is found\n",
    "        return string[:target_index] + target_phrase\n",
    "    else:\n",
    "        return string \n",
    "    \n",
    "def extract_response_content(string, target_phrase):\n",
    "    response_index = string.find(target_phrase)\n",
    "    return string[response_index + len(target_phrase):].strip()\n",
    "\n",
    "def group_examples_random(ds, n): #this is where we group examples into a larger prompt\n",
    "    random.seed()\n",
    "    samples = random.sample(ds, n)\n",
    "    new_prompt = \"\"\n",
    "    for i in range(n):\n",
    "        new_prompt += samples[i]\n",
    "        new_prompt += \"\\n\"\n",
    "    return new_prompt\n",
    "\n",
    "def create_similarity_dict(prompt, train_ds, n_egs=5, target_phrase=\"\\n ### Targets:\"):\n",
    "    similarity_dict = {}\n",
    "    for eg in train_ds:\n",
    "        similarity_dict[eg] = sent_similarity(prompt, select_characters_before_target(eg, target_phrase))\n",
    "    sorted_dict = sorted(similarity_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_egs = []\n",
    "    for item in sorted_dict[:n_egs]:\n",
    "        top_egs.append(item[0])\n",
    "    return top_egs\n",
    "\n",
    "\n",
    "def group_by_similarity(prompt, ds, n_egs, m_choices, target_phrase=\"\\n ### Targets:\"):\n",
    "    choices = random.sample(ds, m_choices)\n",
    "    cos_sim_dict = {}\n",
    "    for c in choices:\n",
    "        cos_sim_dict[c] = sent_similarity(prompt, select_characters_before_target(c, target_phrase))\n",
    "\n",
    "    sorted_cos_sim = sorted(cos_sim_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_egs = \"\"\n",
    "    for item in sorted_cos_sim[:n_egs]:\n",
    "        top_egs += item[0]\n",
    "        top_egs += \"\\n\"\n",
    "\n",
    "    # top_egs = \"\".join([item[0] for item in sorted_cos_sim[:n_egs]])\n",
    "    return top_egs\n",
    "\n",
    "def count_tokens(tokenizer, prompt):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    return len(input_ids)\n",
    "\n",
    "\n",
    "def evaluate_example(model, tokenizer, prompt, model_name, max_tokens):\n",
    "    if model_name == 'gpt2':\n",
    "        num_tokens = count_tokens(tokenizer, prompt)\n",
    "        if num_tokens >= 900:\n",
    "            return None\n",
    "        print(max_tokens, num_tokens)\n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs =model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=max_tokens) #set max_length to 1024 since GPT2 doesnt take nearly as long with ICL\n",
    "        decoded_output = tokenizer.decode(outputs[0][len(model_inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "        return decoded_output\n",
    "    elif model_name == 'mistral':\n",
    "        # num_tokens = count_tokens(tokenizer, prompt)\n",
    "        # print(\"Num tokens in prompt: \", num_tokens)\n",
    "        # if num_tokens > 3400:\n",
    "        #     return None\n",
    "        model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        print(\"Max number of tokens: \", max_tokens)\n",
    "        outputs =model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, do_sample=True, max_new_tokens=max_tokens)\n",
    "        decoded_output = tokenizer.decode(outputs[0][len(model_inputs['input_ids'][0]):], skip_special_tokens=True) #only get output\n",
    "        return decoded_output\n",
    "  \n",
    "\n",
    "def evaluate_icl(train_dataset, test_dataset, model, tokenizer, num_egs, model_name, ds_name='ni', method='similarity', max_tokens_dict=None):\n",
    "    reals = []\n",
    "    preds = []\n",
    "    counter = 0\n",
    "    for example in test_dataset:\n",
    "        # prompt = group_examples(train_dataset, num_egs) + f\"### Question: {example['input']} \\n ###Targets:\"\n",
    "        target_phrase = \"\\n ### Targets:\"\n",
    "        if ds_name == 'ni':\n",
    "            curr_prompt = f\"### Task: {example['definition']}\\n ### Inputs: {example['inputs']}\\n ### Targets:\"\n",
    "            real = f\"{example['targets']}\"\n",
    "            max_tokens = max_tokens_dict[real] + 100\n",
    "        elif ds_name == 'medmcq':\n",
    "            curr_prompt = f\"### Task: {example['instruction']}\\n ### Question: {example['input']}\\n ### Targets:\"\n",
    "            real = f\"{example['output']}\"\n",
    "            tokens = tokenizer(real, return_tensors='pt').to(device)\n",
    "            max_tokens = len(tokens['input_ids'][0]) +  100\n",
    "        elif ds_name == 'finance_sent':\n",
    "            curr_prompt = f\"### Text: {example['text']}\\n ### Targets:\"\n",
    "            real = f\"{example['label']}\"\n",
    "            tokens = tokenizer(real, return_tensors='pt').to(device)\n",
    "            max_tokens = len(tokens['input_ids'][0])\n",
    "        elif ds_name == 'medqa':\n",
    "            curr_prompt = select_characters_before_target(example['text'], \"### Response:\")\n",
    "            real = extract_response_content(example['text'], \"### Response:\")\n",
    "            max_tokens = max_tokens_dict[real] + 100\n",
    "        elif ds_name == 'lawqa':\n",
    "            curr_prompt = f\"### Question: {example['question']}\\n ### Answer:\"\n",
    "            real = example['answer']\n",
    "            max_tokens = max_tokens_dict[real] + 100\n",
    "            target_phrase = \"### Answer:\"\n",
    "        elif ds_name == 'alpaca':\n",
    "            curr_prompt = f\"### Instruction: {example['instruction']}\\n ### Input: {example['input']}\\n ### Text: {example['text']}\\n ### Output:\"\n",
    "            real = example['output']\n",
    "            max_tokens = max_tokens_dict[real]+100\n",
    "            target_phrase=\"### Output:\"\n",
    "\n",
    "        # tokens = tokenizer(real, return_tensors='pt').to(device)\n",
    "        # max_tokens = len(tokens['input_ids'][0])\n",
    "    \n",
    "        if method == 'similarity':\n",
    "            icl_prompt = group_by_similarity(curr_prompt, train_dataset, num_egs, 500, target_phrase) + curr_prompt\n",
    "        elif method == 'random':\n",
    "            icl_prompt = group_examples_random(train_dataset, num_egs) + curr_prompt\n",
    "\n",
    "        # print(\"MAX TOKENS:\\n\", max_tokens)\n",
    "        # print(\"\\n ICL Prompt: \",icl_prompt)\n",
    "        print(\"ICL prompt complete\")\n",
    "        pred = evaluate_example(model, tokenizer, icl_prompt, model_name, max_tokens)\n",
    "        print(\"Prediction complete\")\n",
    "\n",
    "        if counter % 50 == 0:\n",
    "            print(\"PROMPT:\\n\", icl_prompt)\n",
    "            print(\"REAL ANSWER:\\n\", real)\n",
    "            print(\"PREDICTION:\\n\", pred)\n",
    "        if pred:\n",
    "            reals.append(real.lower())\n",
    "            preds.append(pred.lower())\n",
    "        counter+=1\n",
    "\n",
    "    return reals, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "tokenizer_mist_8= AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model_mist_8 = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",  load_in_8bit=True, device_map='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_plain =  GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "# tokenizer_plain = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# print(\"models retrieved\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "load_in_4bit=True,\n",
    "bnb_4bit_use_double_quant=True,\n",
    "bnb_4bit_quant_type=\"nf4\",\n",
    "bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model_mist = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer_mist = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_gpt2=  GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_example2(model, tokenizer, prompt):\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    # print(prompt)\n",
    "    # if len(tokenized_prompt['input_ids'][0]) > MAX_LENGTH: #currently just checking if random prompt is too big or not\n",
    "    #     return None \n",
    "    outputs =model.generate(**model_inputs, pad_token_id= tokenizer.eos_token_id, do_sample=False, max_new_tokens = 5)\n",
    "    decoded_output = tokenizer.decode(outputs[0][len(model_inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    # print(\"prediction: \",decoded_output)/\n",
    "    return decoded_output\n",
    "\n",
    "\n",
    "prompt = \"\"\" \"featuring an oscar-worthy performance => positive\\n\"\n",
    "    \"completely messed up => negative\\n\"\n",
    "    \"masterpiece => positive\\n\"\n",
    "    \"the action is stilted => negative\\n\"\n",
    "    \"by far the worst movie of the year =>\" \"\"\"\n",
    "pred = evaluate_example2(model_mist_8, tokenizer_mist_8, prompt) \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Natural Instructions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zakit\\AppData\\Local\\Temp\\ipykernel_34132\\4232458944.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_test_dataset = test_dataset.to_pandas().groupby('task_name').apply(lambda x: x.head(10)).reset_index(drop=True)\n",
      "Filter: 100%|██████████| 575481/575481 [03:41<00:00, 2593.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set:  100\n",
      "Length of train set 475283\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.\n",
      " ### Inputs: Sentence: Jerry goes out to the pier and casts his favorite bait : cheese . \n",
      "Question: How much time did Jerry spend at the pier?\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " No.\n",
      "PREDICTION:\n",
      " yes\n",
      "\n",
      "### Questions:\n",
      "\t1. How much time did Jerry spend at the pier?\n",
      "\t2. What animal is Jerry’s best friend?\n",
      "\t3. How does Jerry catch fish?\n",
      "\t4. What is the most delicious bait to use in catching fish?\n",
      "\t5. How does Jerry catch fish?\n",
      "\t6. What makes great bait for fishing?\n",
      "\t7. What makes great cheese?\n",
      "\t8. What is the best way to get your\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  128\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  117\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  123\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  127\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: Generate an overlapping word between the given two sentences. When you find the overlapping words, they don't have to match exactly, e.g., \"survival\" and \"survive\" are valid overlapping words. Little words like \"the\" or \"of\" don't count! You must generate significant words which are not the stop words.\n",
      " ### Inputs: Sentence1: Some parasites kill their host, but most do not. \n",
      "Sentence2: All parasites are harmful to their host, but some are beneficial to humans.\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " host\n",
      "PREDICTION:\n",
      " There is an overlapping word between these two sentences! What is the word?\n",
      "\n",
      " \t**Note:** There may be more than one overlapping word between the two sentences. Consider it as a homework for a student to solve, who doesn't know what is the overlapping word between the two sentences.\n",
      "\n",
      " - Your program only considers word2 which is inside the given set of sentences (given in the input).\n",
      " - Word2 is not present in sentence1 (in our case\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  113\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: In this task, you are given two phrases: Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether, as a result of the Head, PersonX wants what is mentioned in the Tail or not. In this task, wanting is a postcondition desire on the part of PersonX, respectively. As a result of PersonX giving PersonY gifts, PersonX may also desire to hug PersonY. Classify your answers into \"Yes\" and \"No\". The phrase may also contain \"___\", a placeholder that can be an object, a person, and/or an action.\n",
      " ### Inputs: Head: PersonX considers closely the ___<sep>Tail: to make a few changes\n",
      " ### Targets: Yes\n",
      "### Task: The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.\n",
      " ### Inputs: Sentence: Jerry goes out to the pier and casts his favorite bait : cheese . \n",
      "Question: How much time did Jerry spend at the pier?\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " No.\n",
      "PREDICTION:\n",
      " Yes\n",
      "### Task: Answer yes if the statement is true, no otherwise.\n",
      "### Inputs: Statement: I'm going home\n",
      "Question: Are you from Australia?\n",
      "### Targets: Yes\n",
      "### Task: Answer yes if the statement is true, no otherwise.\n",
      "### Inputs: Statement: I'm going shopping at IKEA store\n",
      "Question: What store are you going to shop in?\n",
      "### Targets: Yes\n",
      "### Task: Answer yes if the statement is true\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  128\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  117\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  123\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  127\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: In this task, you are given a question. Your task is to generate an answer that is relevant to the question.\n",
      " ### Inputs: Do you like crowds?\n",
      " ### Targets: I'm not keen on crowds.\n",
      "### Task: Generate an overlapping word between the given two sentences. When you find the overlapping words, they don't have to match exactly, e.g., \"survival\" and \"survive\" are valid overlapping words. Little words like \"the\" or \"of\" don't count! You must generate significant words which are not the stop words.\n",
      " ### Inputs: Sentence1: Some parasites kill their host, but most do not. \n",
      "Sentence2: All parasites are harmful to their host, but some are beneficial to humans.\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " host\n",
      "PREDICTION:\n",
      " All\n",
      " ### Task: In this task, you will be tested on how long you can remember a word after a set of distractor words.\n",
      "### Inputs: 13562\n",
      " ### Targets: 68\n",
      "\n",
      " ### Task: In this task, you are tested to determine if a given number sequence contains a sequence of consecutive numbers.\n",
      " ### Inputs: 3, 4, 15, 21, 45, 91\n",
      "\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  113\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: In this task, you are given an input list A. You need to extract and sort the unique digits used in the list in ascending order. Return -1 if there is no digit in the list.\n",
      " ### Inputs: ['g', '473', '45']\n",
      " ### Targets: 3, 4, 5, 7\n",
      "### Task: In this task, you are given a tuple, comprising Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether, as a result of the Head, PersonX will be seen as what is mentioned in the Tail or not. In this task, PersonX will be seen as the Tail if the Tail describes PersonX's persona or attribute as perceived by others given an event. In the gift-giving example, X may be seen as generous or giving. In contrast, in an event such as PersonX steals a car, PersonX may be perceived as evil. Classify your answers into \"Yes\" and \"No\". The phrase may also contain \"___\", a placeholder that can be an object, a person, and/or an action.\n",
      " ### Inputs: Head: PersonX declares ___ on japan<sep>Tail: furious\n",
      " ### Targets: Yes\n",
      "### Task: The answer will be 'yes' if the provided sentence contains an explicit mention that answers the given question. Otherwise, the answer should be 'no'. Instances where the answer is implied from the sentence using \"instinct\" or \"common sense\" (as opposed to being written explicitly in the sentence) should be labeled as 'no'.\n",
      " ### Inputs: Sentence: Jerry goes out to the pier and casts his favorite bait : cheese . \n",
      "Question: How much time did Jerry spend at the pier?\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " No.\n",
      "PREDICTION:\n",
      " yes\n",
      "### Key: In this task, you are given an input text (with context) and a list of words that are of interest. All these words are given in one line, separated by commas. You have to determine if they are mentioned in the given text or not. You have to output a single character for all the words, i.e., '+', if the word is present, or '-' otherwise. \n",
      " ### Inputs: Jerry did not spend much time in the shop\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  128\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  117\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  123\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  127\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Task: You are given a math word problem and you are supposed to apply a single mathematical operators like addition, subtraction, multiplication or division on the numbers embedded in the text to answer the following question and then only report final the numerical answer.\n",
      " ### Inputs: There are 54 scissors in the drawer . Keith placed 22 more scissors in the drawer . How many scissors are now there in all ?\n",
      " ### Targets: 76\n",
      "### Task: You are given an array of integers, check if it is monotonic or not. If the array is monotonic, then return 1, else return 2. An array is monotonic if it is either monotonically increasing or monotonocally decreasing. An array is monotonically increasing/decreasing if its elements increase/decrease as we move from left to right\n",
      " ### Inputs: [38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119]\n",
      " ### Targets: 1\n",
      "### Task: Generate an overlapping word between the given two sentences. When you find the overlapping words, they don't have to match exactly, e.g., \"survival\" and \"survive\" are valid overlapping words. Little words like \"the\" or \"of\" don't count! You must generate significant words which are not the stop words.\n",
      " ### Inputs: Sentence1: Some parasites kill their host, but most do not. \n",
      "Sentence2: All parasites are harmful to their host, but some are beneficial to humans.\n",
      " ### Targets:\n",
      "REAL ANSWER:\n",
      " host\n",
      "PREDICTION:\n",
      " parasites,host,parasites,kill,kill,not,kill,kill,host,kill,host,kill,not.kill,human,host\n",
      "### Task: You are given a string s with the digits(0-9), lower case letters  and some special characters. It can be assumed that there are no duplicate characters and no adjacent same characters in the string. Segment the string according to the following rules -\n",
      "    (1) When you see a special character\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  110\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  113\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = tokenizer_mist\n",
    "model = model_mist\n",
    "\n",
    "data = load_from_disk('data/1000_per_task')\n",
    "\n",
    "# data = filter_icl(data, max_num_egs, tokenizer_plain)\n",
    "\n",
    "max_num_egs =  3   #natural instructions are just too big\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "# train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "# train_dataset = train_test_split['train']\n",
    "# test_dataset = train_test_split['test']\n",
    "\n",
    "train_dataset = data['train']\n",
    "test_dataset = data['test']\n",
    "\n",
    "grouped_test_dataset = test_dataset.to_pandas().groupby('task_name').apply(lambda x: x.head(10)).reset_index(drop=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(grouped_test_dataset.head(100))\n",
    "\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'ni'\n",
    "\n",
    "def filter_example(example):\n",
    "    return count_tokens(tokenizer, f\"### Task: {example['definition']}\\n ### Inputs: {example['inputs']}\\n ### Targets: {example['targets']}\") <= 300\n",
    "\n",
    "train_dataset = train_dataset.filter(filter_example)\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name=\"ni\")\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "max_token_dict = {}\n",
    "for eg in test_dataset:\n",
    "    max_token_dict[eg['targets']] = count_tokens(tokenizer, eg['targets'])\n",
    "\n",
    "# csv_file = f'icl_results/similarity_dicts_{ds_name}.csv'\n",
    "# if os.path.isfile(csv_file):\n",
    "#     df = pd.read_csv(csv_file)\n",
    "#     test_similarity_dict = dict(zip(df['Prompt'], df['Similar_Prompts']))\n",
    "# else:\n",
    "#     test_similarity_dict = {}\n",
    "#     for eg in test_dataset:\n",
    "#         prompt = f\"### Task: {eg['definition']}\\n ### Inputs: {eg['inputs']}\\n ### Targets:\"\n",
    "#         test_similarity_dict[eg] = create_similarity_dict(prompt, train_list)\n",
    "#     print(test_similarity_dict)\n",
    "#     df = pd.DataFrame(list(test_similarity_dict.items()), columns=['Prompt', 'Similar_Prompts'])\n",
    "#     df.to_csv(csv_file, index=False)\n",
    "\n",
    "# print(test_similarity_dict)\n",
    "\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "for i in range(max_num_egs):\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model, tokenizer, i, model_name=model_name, ds_name=ds_name, method=icl_method, max_tokens_dict=max_token_dict)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order= order) #set order to mean of real values\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations':i, 'bert_score' : float(average_F1), 'bleu_score': bleu_score['bleu']})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/{icl_method}/icl_results_{ds_name}_{model_name}.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Alpaca Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Filter: 100%|██████████| 52002/52002 [00:19<00:00, 2627.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set:  100\n",
      "Length of train set 33633\n",
      "{'The maximum number of steps in a Fibonacci sequence is 93.': 18, 'An example of the Second Law of Thermodynamics is when heat flows from a hot object to a cold object. The hot object cools down, and the cold object warms up, but the energy is not conserved and some of the heat energy is lost.': 56, \"It's time to get ready for the thrilling new movie! Get ready to experience edge-of-your-seat suspense, nail-biting suspense, and clever twists. #MovieNight #UpcomingMovie\": 52, 'Prime numbers: 5, 31\\nComposite numbers: 16, 9, 18': 25, 'Simile.': 4, 'A good exercise routine should incorporate aerobic exercise, strength training, and flexibility exercises. Examples of aerobic activities include walking, jogging, biking, or swimming. Strength training exercises include lifting weights, using resistance bands and bodyweight exercises. Flexibility exercises include stretching and yoga. A well structured routine should also have rest and recovery days.': 75, '^[A-Z][a-z]+$': 12, 'The potential issue with the argument is that it does not take into account any potential disadvantages of raising the minimum wage. There may be unintended consequences such as businesses raising prices to offset their increased labour costs, resulting in higher cost of living for the consumer. This could ultimately lead to decreased demand for goods and services, resulting in fewer jobs available for the workers in the long run.': 81, 'We had a good time!': 7, 'How about a weekend getaway to the town of Mendocino in California? Spend the first day exploring the stunning cliffs overlooking the Pacific Ocean and kayaking in the calm waters. Spend the second day exploring the charming village, visiting the top-notch wineries, and doing some light shopping and souvenir finding. End the weekend with a romantic dinner at Café Beaujolais and catching the breathtaking sunsets from the bluffs.': 100, 'Five other methods to measure air quality include:\\n- monitoring of ambient air concentrations and emissions\\n- remote sensing techniques (e.g. satellite imaging)\\n- establishing regional monitoring networks\\n- collecting and analyzing in-situ samples \\n- measurement of deposition and aerosols': 64, '|  Column 1  |  Column 2  |  Column 3  |\\n|:----------:|:----------:|:----------:|\\n|      1     |      4     |      7     |\\n|      2     |      5     |      8     |\\n|      3     |      6     |      9     |\\n|            |      10    |            |': 87, 'From self-driving cars to robot surgeons - #artificialintelligence is revolutionizing our lives! #AI #FutureTech': 29, 'Some common elements found in horror movies are suspense, dark atmosphere, jump scares, mysterious and terrifying characters, unexpected twists, and graphic violence and gore.': 36, 'A system should be created for amplifying user feedback to ensure that users’ ideas and suggestions are heard and taken into account. The system should involve collecting user feedback from various sources, including surveys, social media, and customer reviews. The collected feedback should then be sorted and organized into categories for easy access. Next, a knowledge database should be created to store and document the feedback that is potentially useful for the organization. Finally, an automated system should be developed to alert the relevant stakeholders whenever new feedback is available.': 104, 'I apologize for missing our appointment and I understand how this might have caused you inconvenience. I take full responsibility for my actions and I promise to do my best to make it up to you.': 42, \"The prefix of 'uncooperative' is 'un-' which means not, and the suffix means 'cooperative'. Therefore, the word 'uncooperative' means 'not cooperative'.\": 44, 'Rock, paper, scissors, shoot! (Rock, Paper or Scissors are the input)': 24, 'The Inventive Pen.': 7, 'Possible activities that one could do in a city include visiting museums, parks, shopping centers, theaters, restaurants, and sightseeing.': 31, 'NicerShoes is an amazing online store with a huge selection of shoes. The prices are reasonable, the delivery is quick and efficient, and the customer service is excellent. I would definitely recommend NicerShoes to anyone looking for good quality, reasonably priced shoes.': 56, 'Flour, Baking powder, Cinnamon, Milk, Large Egg': 18, 'Visual interface, user experience, performance and scalability, security, testing, and compatibility with different devices.': 22, \"J'ai besoin d'aide.\": 11, 'False. Cats can taste sweet food, but they typically prefer savory sources of flavor.': 20, 'The sound was very loud.': 7, \"Gender equality is a major theme in the book 'A Doll's House.'\": 19, 'The use of social media has a profound impact on society both positively and negatively, with its applications ranging from facilitating communication to creating divisions within communities.': 33, '5 x 8 = 40, so the equation would be (5 x 8) + 10 = 50.': 31, 'One job where excellent problem-solving skills would be necessary is a software engineer. Software engineers need to be able to evaluate a problem, analyze the requirements, and develop an efficient solution using the appropriate programming languages and technologies. They must also possess the ability to think logically and devise creative solutions to any challenge they encounter.': 67, '1. N-Gram Modeling\\n2. Part-of-Speech Tagging\\n3. Constituency Parsing\\n4. Dependency Parsing\\n5. Named Entity Recognition\\n6. Word Embedding\\n7. Topic Modeling\\n8. Machine Translation\\n9. Text Summarization\\n10. Text-to-Speech Synthesis.': 87, 'Treatment  |  Effectiveness  | \\n-----------------------------------\\nTreatment 1  |  x  | \\nTreatment 2  |  y  | \\nTreatment 3  |  z  | \\nTreatment 4  |  a  | \\nTreatment 5  |  b  |': 79, 'Mount Everest (8848m), K2 (8611m), and Kangchenjunga (8586m).': 34, 'Sustainable development is a concept which advocates for the development of economies and societies in a way that meets present needs and needs of the future generations without compromising the natural resources and environment. It promotes the idea of using resources in a way that causes less damage to the environment and also encourages conserving natural resources through proper management, efficient use of resources, and renewable resources.': 81, 'Tranquility, serenity.': 9, 'Wind: Clarinet, Oboe\\nString: Violin': 14, 'Water cannot exist in a vacuum.': 8, 'I am powerful and capable of achieving anything I set my mind to.': 15, 'The recommended method for implementing password resets is by using a one-time token, also known as a one-time password (OTP). This token should be sent to the user via email or text message, and should be used to authenticate the user before allowing them to reset their password.': 62, 'Shopping list: cereal, 2% milk, large eggs, and extra large eggs.': 21, 'Mayor Accused of Embezzling Funds From City Coffers': 15, 'A possible diagnosis is abdmoninal pain caused by gastritis.': 16, 'Life is like the epic story of Star Wars: you will face obstacles, make tough decisions, and forge your own path. No matter what, you can always choose the path of courage and hope, and never give up.': 48, 'Tall and leafy.': 6, 'Machine learning is a type of artificial intelligence that focuses on building computer systems that learn by themselves. These systems don’t require explicit programming to create models or perform tasks; instead, they use large datasets to make predictions and automate tasks. By ingesting data and mining patterns, machine learning algorithms can detect complex relationships and iteratively improve their predictive accuracy.': 74, 'The cat was awake restlessly in the sun.': 11, 'She peered out the window.': 8, 'Sometimes I just need a jump start.': 9, 'Nature writing': 3, 'Here are three food items you can prepare with the ingredients in your kitchen:\\n1. Tomato, bell pepper and lettuce salad. \\n2. Grilled carrots with bell peppers and tomatoes. \\n3. Tomato and bell pepper soup.': 58, 'I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to do our part to reduce our consumption and waste, as well as to find ways to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.': 89, 'A living algorithm is a model of a physical system that continuously learns from its environment in order to optimize its behaviors. It is an evolutionary algorithm that can adapt and self-improve in unpredictable environments. These algorithms are inspired by nature and are used to solve complex optimization tasks that are difficult for traditional algorithmic approaches.': 70, 'The code has one mistake. The \"i++\" line should not be indented, since it is not part of the for loop.': 29, 'I will build a predictive model using machine learning algorithms, such as naïve bayes, support vector machines, and random forests, based on the available customer demographic, purchase history and customer preference data. The model will help predict the likelihood of customer purchase.': 55, 'zxcvb932': 7, 'if (num1 > num2) {\\n  console.log(num1 + \" is greater than \" + num2);\\n} else {\\n  console.log(num1 + \" is not greater than \" + num2);\\n}': 53, 'The student wrote the thesis.': 7, 'A business is like a finely tuned machine. Just like a machine, it needs visionary leaders, dependable workers and well-oiled resources in order to run efficiently and produce successful results. Each individual has their role to play, and when everyone works together in harmony, the company flourishes and succeeds.': 66, 'A teapot - ceramic, intricately decorated, golden.': 16, 'The students have rapidly improved their literacy over the last year.': 14, 'The illusion of a glass of water is a visual trick that makes it appear as if the glass is half full or half empty, when in reality it is the same amount in both halves. This illusion has been studied extensively in the context of perception and cognition.': 56, 'I have been blissfully residing here for many years.': 14, 'Yes': 2, 'A successful business idea should have these characteristics: a unique selling proposition, the ability to meet a market need, potential for scalability, capital-efficiency, and realistic market potential. Additionally, the business should be able to be executed within a given budget and timeframe and should be able to provide a return on investment for its investors.': 69, 'A suitable topic for a presentation about the petroleum industry could be \"The Impact of the Petroleum Industry on the Global Economy\". This topic would explore how the oil and gas industry affects the global economy, covering topics such as job creation and economic growth, contribution to GDP, export of oil, and resource conservation. The presentation could also discuss how the industry is responding to the rise of renewable energy sources, and how this change will affect global markets.': 96, '[Start] \\n|\\nV\\nFill out job application \\n|\\nV\\nAttach relevant documents \\n|\\nV\\nSchedule job interview \\n|\\nV \\nAttend job interview\\n|\\nV\\nReceive job offer\\n|\\nV\\n[End]': 61, 'The thick canopy of the rainforest blocked out almost all of the sunlight, leaving the ground in perpetual shade. The air was damp and humid, and the humidity made it feel like the air was thick and viscous. It was as if breathing had become an effort in itself. Even the animals seemed to be holding their breath. Suddenly, a loud crash filled the air, and then all was still.': 88, '\"We all make mistakes sometimes, so please don\\'t feel too bad about this one.\"': 20, \"Creating a website for a local business requires a number of steps, including designing the site's layout and structure, writing content for the homepage and individual pages, incorporating features such as maps, contact forms, and appointment requests, and optimizing the site for search engines. Additionally, if the business is using the website to generate business, features such as e-commerce and payment processing need to be implemented. Lastly, the website must be tested and secured to ensure that the business's information is kept safe and the website is accessible to all potential customers.\": 116, 'Good customer service should be friendly, professional, timely, courteous, and informative.': 21, 'The pair of numbers with the greatest product is 0 and -4, which has a product of 0.': 24, \"One way for college students to save money is to take advantage of student discounts wherever possible. Many stores and online retailers offer discounts for students, so it's worth taking the time to look for them when making a purchase. Additionally, learning to budget and tracking spending is a great way to save money. Setting up a budget and then tracking expenses by noting all purchases helps to stay organized and keep expenses in check.\": 87, 'I feel abandoned and hollow, like a ship without its anchor.': 14, 'The job with the highest hourly pay rate is a surgeon, with an average hourly rate of $97.46.': 29, 'World War I, which was fought between 1914 and 1918, is considered to be the closest in terms of casualties, physical destruction, and global impacts. The two wars were instigated by aggressors and sparked off escalating hostilities which led to the eventual involvement of the major world powers at the time. Both wars featured heavily mechanized warfare and unprecedented mobilization of soldiers and resources resulting in unprecedented loss of life and destruction.': 103, 'Root Mean Squared Error (RMSE) is an error metric used in machine learning and data science to evaluate how well a model is performing on a given dataset. It measures the average absolute difference between the actual and predicted values. RMSE is the square root of the average squared difference between the predicted and actual values. It punishes large errors more heavily than small errors, making it a better measure for model performance than mean absolute error.': 92, '\"Fam\" is a popular urban slang term used to refer to one\\'s close friends.': 22, '- Host an online or in-person event to provide information and demonstrations of products or services. \\n- Launch a promotional campaign offering trial or discount offers to give potential customers the opportunity to experience the product or service.\\n- Create a referral program where customers who refer new customers receive an incentive.': 66, 'In my region, there are many pine trees. The most common types of pine trees are Eastern White Pine, Red Pine, and Jack Pine. These trees grow to a height of between 50-90 feet, and their needles are between 5-10 centimeters long. The larvae of a few species of moths feed on pine needles, and pinecones also provide food to birds and small mammals.': 98, 'The French Revolution was a period of political and social upheaval in France from 1789 to 1799.': 29, 'The three main symptoms of Coronavirus are: 1) Cough 2) Fever 3) Shortness of Breath or difficulty breathing. Other less common symptoms include fatigue, headache, sore throat, and loss of smell or taste.': 54, 'The five countries in Europe that have a monarchy are the United Kingdom, Andorra, Belgium, Monaco, and Liechtenstein.': 30, '181': 5, 'Observation.': 4, 'The mood of the text is foreboding.': 11, 'Setting up a local network requires understanding the different components involved. First, you need to check if your router supports a local network. Then, you need to determine the types of connections you need, such as wireless or direct. After that, you need to connect the router to the internet, if required. Once you have the router connected, you need to configure the network name and assign IP addresses. Finally, if desired, you can add additional security precautions to the network.': 98, 'Classify recipes into one of five categories: appetizers, entrees, desserts, drinks, and snacks.': 25, 'Verbs: walk\\nNouns: tree, cat, river': 15, 'A professional chef will cut a mango with a sharp knife, beginning by slicing off the stem end. Using the same knife, they will slice the mango lengthwise, getting as close to the pit as possible. Then the chef will use a spoon to scoop out the flesh. They can use the slices for dicing or cubes, or just enjoy the half without cutting further.': 82, 'The postman was singing while he walked in the late night traffic. He had an envelope tucked under his arm which he delivered to a kitchen on the way.': 33, 'Three marketing strategies could include digital marketing (such as SEO, social media, and email marketing), content marketing (such as blogging, webinars, and podcasts), and PR (such as press releases, media appearances, and event planning).': 51, 'def add_two_numbers(a, b):\\n    return a + b': 19, 'John (Present Simple: like) likes (Present Simple: like) the (No verb) blue (No verb) house (No verb) at (No verb) the (No verb) end (No verb) of (No verb) the (No verb) street (No verb).': 60, 'Archaeologist': 4, 'Small, tiny, diminutive': 8, 'The sentence is using a metaphor to compare the wind to a master.': 16, \"There once was a girl from the lake\\nWho enjoyed skating for fun 'n for sake\\nShe was a pro at the sport\\nAnd it quickly became her forte\\nAs she zipped around without a break!\": 46, 'Computer viruses are like a forest fire: both rapidly spread and can cause immense damage if not contained quickly.': 23, 'Ambition is like a lion fiercely driven to reach the pinnacle of success.': 20, 'Expressive, passionate, adventurous, reliable, and generous.': 15}\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Instruction: What is the maximum number of steps in a Fibonacci sequence?\n",
      " ### Input: No input.\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the maximum number of steps in a Fibonacci sequence?\n",
      "\n",
      "### Input:\n",
      "No input.\n",
      "\n",
      "### Response:\n",
      "The maximum number of steps in a Fibonacci sequence is 93.\n",
      " ### Output:\n",
      "REAL ANSWER:\n",
      " The maximum number of steps in a Fibonacci sequence is 93.\n",
      "PREDICTION:\n",
      " \n",
      "\n",
      "### Response:\n",
      "\n",
      "The maximum number of steps in a Fibonacci sequence is 93.\n",
      "\n",
      "###  Instruction:\n",
      "What is the smallest index of a Fibonacci sequence where the number is not even?\n",
      "\n",
      "### Input:\n",
      "No input.\n",
      "\n",
      "### Response:\n",
      "The smallest index of a Fibonacci sequence in which the number is not even is 42.\n",
      "### Output:\n",
      "\n",
      "### Response:\n",
      "The smallest index of a Fibonacci sequence in which the number is not even is 4\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  152\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  112\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  181\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  200\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  164\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  204\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  142\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  144\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  131\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  133\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  131\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  167\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  179\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  134\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  181\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  109\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  162\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  148\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  174\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  109\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  158\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  189\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Instruction: What is your opinion about the given concept?\n",
      " ### Input: Living a sustainable lifestyle\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is your opinion about the given concept?\n",
      "\n",
      "### Input:\n",
      "Living a sustainable lifestyle\n",
      "\n",
      "### Response:\n",
      "I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to do our part to reduce our consumption and waste, as well as to find ways to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.\n",
      " ### Output:\n",
      "REAL ANSWER:\n",
      " I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to do our part to reduce our consumption and waste, as well as to find ways to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.\n",
      "PREDICTION:\n",
      " \n",
      " I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to reduce our consumption and waste and to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.\n",
      "\n",
      " ### Explanation:\n",
      " The response above appropriately completes the request by providing a balanced assessment of the concept of living a sustainable lifestyle. The response acknowledges the importance of reducing consumption and waste, using renewable energy sources, and considering environmental and social factors when making purchases.\n",
      "\n",
      " ### Evaluation:\n",
      " The response above is a high-quality output as it is complete and addresses the request. It provides a well-rounded and balanced assessment of the concept of living a sustainable lifestyle, acknowledges the importance of reducing waste\n",
      "ICL prompt complete\n",
      "Max number of tokens:  170\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  155\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  153\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  166\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  169\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  196\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  161\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  188\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  216\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  203\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  192\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  166\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  154\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  130\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  133\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  151\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  160\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  146\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  123\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Instruction: Analyze the following sentence and classify it as a complete sentence, fragment, or a run-on sentence.\n",
      " ### Input: I want to eat breakfast.\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Analyze the following sentence and classify it as a complete sentence, fragment, or a run-on sentence.\n",
      "\n",
      "### Input:\n",
      "I want to eat breakfast.\n",
      "\n",
      "### Response:\n",
      "Complete sentence. \n",
      " ### Output: Complete sentence.\n",
      "### Instruction: What is the maximum number of steps in a Fibonacci sequence?\n",
      " ### Input: No input.\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the maximum number of steps in a Fibonacci sequence?\n",
      "\n",
      "### Input:\n",
      "No input.\n",
      "\n",
      "### Response:\n",
      "The maximum number of steps in a Fibonacci sequence is 93.\n",
      " ### Output:\n",
      "REAL ANSWER:\n",
      " The maximum number of steps in a Fibonacci sequence is 93.\n",
      "PREDICTION:\n",
      " The maximum number of steps in a Fibonacci sequence is 93.\n",
      "### Instruction: List all of the odd integers between one and ten.\n",
      " ### Input: No input.\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "List all of the odd integers between one and ten.\n",
      "\n",
      "### Input:\n",
      "No input.\n",
      "\n",
      "### Response:\n",
      "There are five odd integers between one and\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  152\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  112\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  181\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  200\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  164\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  204\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  142\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  144\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  131\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  133\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  131\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  167\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  179\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  134\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  181\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  109\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  162\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  148\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  174\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  109\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  158\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  189\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Instruction: Sort a list of items alphabetically.\n",
      " ### Input: apple, banana, orange, grape\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort a list of items alphabetically.\n",
      "\n",
      "### Input:\n",
      "apple, banana, orange, grape\n",
      "\n",
      "### Response:\n",
      "apple, banana, grape, orange \n",
      " ### Output: apple, banana, grape, orange\n",
      "### Instruction: What is your opinion about the given concept?\n",
      " ### Input: Living a sustainable lifestyle\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is your opinion about the given concept?\n",
      "\n",
      "### Input:\n",
      "Living a sustainable lifestyle\n",
      "\n",
      "### Response:\n",
      "I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to do our part to reduce our consumption and waste, as well as to find ways to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.\n",
      " ### Output:\n",
      "REAL ANSWER:\n",
      " I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to do our part to reduce our consumption and waste, as well as to find ways to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.\n",
      "PREDICTION:\n",
      " I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to do our part to reduce our consumption and waste, as well as to find ways to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.\n",
      "\n",
      "\n",
      " ### Instruction: What is your opinion about the following article?\n",
      " ### Text:\n",
      " ### Instruction:\n",
      "What is your opinion about the following article? \n",
      "### Input:\n",
      "The Most Surprising Things About China\n",
      "\n",
      "### Response:\n",
      "I found the article \"The Most Surprising Things About China\" to be a very interesting and informative read. It provided a great overview of China's culture and history, and I was particularly surprised to learn about the\n",
      "ICL prompt complete\n",
      "Max number of tokens:  170\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  155\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  153\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  166\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  169\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  196\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  161\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  188\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  216\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  203\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  192\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  166\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  154\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  130\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  133\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  151\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  160\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  146\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  123\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Instruction: Name two famous baseball players.\n",
      " ### Input: \n",
      " ### Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name two famous baseball players.\n",
      "\n",
      "### Response:\n",
      "Babe Ruth and Derek Jeter. \n",
      " ### Output: Babe Ruth and Derek Jeter.\n",
      "### Instruction: Create an algorithm to convert traditional currency amounts to Bitcoin.\n",
      " ### Input: \n",
      " ### Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Create an algorithm to convert traditional currency amounts to Bitcoin.\n",
      "\n",
      "### Response:\n",
      "The algorithm should take into account the current exchange rate for the traditional currency and inputted amount, then convert the amount to Bitcoin at the current rate of exchange. For example, if the current exchange rate is USD to BTC at 1 BTC to $8,000, then $100 would be equal to 0.0125 BTC. The algorithm should also have error handling for any incorrect inputs given. \n",
      " ### Output: The algorithm should take into account the current exchange rate for the traditional currency and inputted amount, then convert the amount to Bitcoin at the current rate of exchange. For example, if the current exchange rate is USD to BTC at 1 BTC to $8,000, then $100 would be equal to 0.0125 BTC. The algorithm should also have error handling for any incorrect inputs given.\n",
      "### Instruction: What is the maximum number of steps in a Fibonacci sequence?\n",
      " ### Input: No input.\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the maximum number of steps in a Fibonacci sequence?\n",
      "\n",
      "### Input:\n",
      "No input.\n",
      "\n",
      "### Response:\n",
      "The maximum number of steps in a Fibonacci sequence is 93.\n",
      " ### Output:\n",
      "REAL ANSWER:\n",
      " The maximum number of steps in a Fibonacci sequence is 93.\n",
      "PREDICTION:\n",
      " The maximum number of steps in a Fibonacci sequence is 93.\n",
      "### Instruction:\n",
      "What are the factors of 900?\n",
      "\n",
      "### Input:\n",
      "No input.\n",
      "\n",
      "### Response:\n",
      "### Output:\n",
      "### Instruction:\n",
      "Say “Good” if the number 3 is received.\n",
      "\n",
      "### Input:\n",
      "“3”\n",
      "\n",
      "### Output:\n",
      "Good\n",
      "### Instruction:\n",
      "Say “Bad” if the number 3 is received.\n",
      "\n",
      "### Input:\n",
      "“3”\n",
      "\n",
      "### Output\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  152\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  175\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  112\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  181\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  200\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  164\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  136\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  204\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  142\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  144\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  131\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  118\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  133\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  131\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  167\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  179\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  134\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  181\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  109\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  162\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  148\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  106\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  174\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  109\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  103\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  158\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  189\n",
      "Prediction complete\n",
      "PROMPT:\n",
      " ### Instruction: List three reasons to be environmentally friendly.\n",
      " ### Input: \n",
      " ### Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "List three reasons to be environmentally friendly.\n",
      "\n",
      "### Response:\n",
      "1. Being environmentally friendly preserves natural resources for future generations. \n",
      "2. It reduces pollution and its negative effects on health and well-being. \n",
      "3. It can lead to cost savings due to reduced energy costs and waste disposal fees. \n",
      " ### Output: 1. Being environmentally friendly preserves natural resources for future generations. \n",
      "2. It reduces pollution and its negative effects on health and well-being. \n",
      "3. It can lead to cost savings due to reduced energy costs and waste disposal fees.\n",
      "### Instruction: Construct a sentence using at least three of the given words.\n",
      " ### Input: Feet, bicycle, aunt\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Construct a sentence using at least three of the given words.\n",
      "\n",
      "### Input:\n",
      "Feet, bicycle, aunt\n",
      "\n",
      "### Response:\n",
      "My aunt and I went for a bicycle ride on a sunny day and our feet felt rejuvenated. \n",
      " ### Output: My aunt and I went for a bicycle ride on a sunny day and our feet felt rejuvenated.\n",
      "### Instruction: What is your opinion about the given concept?\n",
      " ### Input: Living a sustainable lifestyle\n",
      " ### Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is your opinion about the given concept?\n",
      "\n",
      "### Input:\n",
      "Living a sustainable lifestyle\n",
      "\n",
      "### Response:\n",
      "I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to do our part to reduce our consumption and waste, as well as to find ways to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.\n",
      " ### Output:\n",
      "REAL ANSWER:\n",
      " I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to do our part to reduce our consumption and waste, as well as to find ways to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.\n",
      "PREDICTION:\n",
      " \n",
      " I believe that living a sustainable lifestyle is an important choice for the future of the planet. It is important to do our part to reduce our consumption and waste, as well as to find ways to use renewable energy sources such as solar, wind, and geothermal. Sustainability also includes reducing our reliance on non-renewable resources and making informed decisions about our purchases with regard to both environmental and social considerations.\n",
      "### Instruction: List the items of a shopping list, and their quantities.\n",
      " ### Input: \n",
      " ### Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "List the items of a shopping list, and their quantities.\n",
      "\n",
      "### Input: \n",
      "\n",
      "### Response:\n",
      "Fresh fruit, 4\n",
      "Vegetables, 5\n",
      "Meat, 3\n",
      " ### Output\n",
      "ICL prompt complete\n",
      "Max number of tokens:  170\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  155\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  153\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  107\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  166\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  156\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  102\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  169\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  196\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  161\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  188\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  216\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  121\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  124\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  187\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  114\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  203\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  192\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  122\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  166\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  129\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  154\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  130\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  105\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  111\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  198\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  125\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  182\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  133\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  151\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  119\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  160\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  104\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  108\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  116\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  146\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  123\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  120\n",
      "Prediction complete\n",
      "ICL prompt complete\n",
      "Max number of tokens:  115\n",
      "Prediction complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_mist\n",
    "model = model_mist\n",
    "\n",
    "data = load_dataset('tatsu-lab/alpaca')['train']\n",
    "\n",
    "# data = filter_icl(data, max_num_egs, tokenizer_plain)\n",
    "\n",
    "max_num_egs =  3   #natural instructions are just too big\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "def filter_example(example):\n",
    "    return count_tokens(tokenizer, f\"### Instruction: {example['instruction']}\\n ### Input: {example['input']}\\n ### Text: {example['text']} \\n ### Output: {example['output']}\") <= 300\n",
    "\n",
    "data = data.filter(filter_example)\n",
    "\n",
    "\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "# train_dataset = data['train']\n",
    "# test_dataset = data['test'].select(range(100))\n",
    "\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'mistral'\n",
    "ds_name = 'alpaca'\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name=\"alpaca\")\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "max_token_dict = {}\n",
    "for eg in test_dataset:\n",
    "    max_token_dict[eg['output']] = count_tokens(tokenizer, eg['output'])\n",
    "print(max_token_dict)\n",
    "\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "for i in range(max_num_egs):\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model, tokenizer, i, model_name=model_name, ds_name=ds_name, method=icl_method, max_tokens_dict=max_token_dict)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order= order) #set order to mean of real values\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations':i, 'bert_score' : float(average_F1), 'bleu_score': bleu_score['bleu']})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(range(max_num_egs), bert_scores) \n",
    "# plt.xlabel('Number of examples') \n",
    "# plt.ylabel('BERT F1 Score') \n",
    "# plt.title('BERT F1 Score vs Number of Examples') \n",
    "# plt.xticks(range(max_num_egs))\n",
    "# plt.savefig('BERT_scores_icl_ni.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(int(sum(len(s) for s in refs)/len(refs)))\n",
    "# test = bleu.compute(predictions=['No', 'Yes', 'Yes', 'yes', 'Yes', 'yes', 'yes', 'No', 'yes', 'No'],\n",
    "#                     references=['No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.'], max_order=int(sum(len(s) for s in refs)/len(refs)))\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Medical MCQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "tokenizer = tokenizer_gpt2\n",
    "model = model_gpt2\n",
    "\n",
    "max_num_egs = 5\n",
    "\n",
    "data = load_dataset('medalpaca/medical_meadow_medqa')['train']\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "def filter_example(example):\n",
    "    return count_tokens(tokenizer, f\"### Task: {example['instruction']}\\n ### Question: {example['input']}\\n ### Targets: {example['output']}\") <= 500\n",
    "\n",
    "train_dataset = train_dataset.filter(filter_example)\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='medmcq')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'gpt2'\n",
    "ds_name = 'medmcq'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_mist, tokenizer_mist, i, model_name=model_name, ds_name=ds_name, method=icl_method)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(\"order: \", order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    "    for r,p in zip(reals, preds):\n",
    "        if len(p.strip()) != 0:\n",
    "            if r.strip()[0] == p.strip()[0]:\n",
    "                accuracy+=1\n",
    "    accuracy = accuracy/len(preds)\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu'], 'accuracy':accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(range(max_num_egs), bert_scores) \n",
    "# plt.xlabel('Number of examples') \n",
    "# plt.ylabel('BERT F1 Score')\n",
    "# plt.title('BERT F1 Score vs Number of Examples') \n",
    "# plt.xticks(range(max_num_egs))\n",
    "# plt.savefig('BERT_scores_icl_medqa.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Transcript Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_num_egs = 5\n",
    "\n",
    "data = load_dataset('jlh-ibm/earnings_call', 'transcript-sentiment')['train']\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "tokenizer = tokenizer_gpt2\n",
    "model = model_gpt2\n",
    "\n",
    "def filter_example(example):\n",
    "    return count_tokens(tokenizer, f\"### Text: {example['text']}\\n ### Targets: {example['label']}\") <= 300\n",
    "\n",
    "train_dataset = train_dataset.filter(filter_example)\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='finance_sent')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'gpt2'\n",
    "ds_name = 'finance_sent'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model, tokenizer, i, model_name=model_name, ds_name=ds_name, method=icl_method)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(\"order: \", order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    "    for r,p in zip(reals, preds):\n",
    "        if len(p.strip()) != 0:\n",
    "            if r.strip()[0] == p.strip()[0]:\n",
    "                accuracy+=1\n",
    "    accuracy = accuracy/len(preds)\n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu'], 'accuracy':accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Medicine QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_num_egs = 3\n",
    "\n",
    "data = load_dataset('Laurent1/MedQuad-MedicalQnADataset_128tokens_max')['train']\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "tokenizer = tokenizer_gpt2\n",
    "model = model_gpt2\n",
    "\n",
    "def filter_example(example):\n",
    "    return count_tokens(tokenizer, extract_response_content(example['text'], \"### Response:\")) <= 300\n",
    "\n",
    "data = data.filter(filter_example)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "\n",
    "max_token_dict = {}\n",
    "for example in test_dataset:\n",
    "    real = extract_response_content(example['text'], \"### Response:\")\n",
    "    max_token_dict[real] = count_tokens(tokenizer, real)\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='medqa')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "# avg_tokens = 0\n",
    "# for eg in train_list:\n",
    "#     avg_tokens+= count_tokens(tokenizer_mist, eg)\n",
    "# avg_tokens  = avg_tokens/len(train_list)\n",
    "# print(\"AVG TOKENS: \",avg_tokens)\n",
    "#avg_tokens = \n",
    "\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'gpt2'\n",
    "ds_name = 'medqa'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model, tokenizer, i, model_name=model_name, ds_name=ds_name, method=icl_method, max_tokens_dict=max_token_dict)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    print(\"order: \", order)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    " \n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu']})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "\n",
    "Mistral 8B?\n",
    "Law dataset\n",
    "Figure out max tokens crap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_num_egs = 3\n",
    "\n",
    "data = load_dataset('dzunggg/legal-qa-v1')['train']\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "tokenizer = tokenizer_gpt2\n",
    "model = model_gpt2\n",
    "\n",
    "def filter_example(example):\n",
    "    return count_tokens(tokenizer, f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\") <= 300\n",
    "\n",
    "data = data.filter(filter_example)\n",
    "\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(100))\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='lawqa')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "max_token_dict = {}\n",
    "for eg in test_dataset:\n",
    "    max_token_dict[eg['answer']] = count_tokens(tokenizer, eg['answer'])\n",
    "\n",
    "# avg_tokens = 0\n",
    "# for eg in train_list:\n",
    "#     avg_tokens+= count_tokens(tokenizer_mist, eg)\n",
    "# avg_tokens  = avg_tokens/len(train_list)\n",
    "# print(\"AVG TOKENS: \",avg_tokens)\n",
    "#avg_tokens = \n",
    "\n",
    "\n",
    "\n",
    "# print(\"Token Dict complete\")\n",
    "\n",
    "icl_method = 'random'\n",
    "model_name = 'gpt2'\n",
    "ds_name = 'lawqa'\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "\n",
    "\n",
    "for i in range(max_num_egs):\n",
    "    accuracy = 0\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model, tokenizer, i, model_name=model_name, ds_name=ds_name, method=icl_method, max_tokens_dict=max_token_dict)\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    refs = [[r] for r in reals]\n",
    "    order = int(sum(len(s) for s in refs)/len(refs))\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs, max_order=order)\n",
    " \n",
    "    results_data.append({'num_samples' : len(preds), 'num_demonstrations' : i, 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu']})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(f'icl_results/icl_results_{ds_name}_{icl_method}_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
