{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICL - GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zakit\\Documents\\COMP0087 CW\\COMP0087-Group\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk, load_metric\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from encodeinstruction import encodeinstruction\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import json\n",
    "import bert_score\n",
    "import evaluate\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def sent_similarity(sent1, sent2):\n",
    "    sentences = [sent1, sent2]\n",
    "    embeddings = sent_model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix[0][1]\n",
    "\n",
    "\n",
    "def format_examples(ds, ds_name='ni'):\n",
    "    prompts = []\n",
    "    if ds_name == 'ni':\n",
    "        for example in ds:\n",
    "            # prompt = f\"### Question: {example['input']} \\n ###Targets: {example['output']}\"\n",
    "            prompt = f\"### Task: {example['definition']}\\n ### Inputs: {example['inputs']} \\n ### Targets: {example['targets']}\"\n",
    "            prompts.append(prompt)\n",
    "    elif ds_name == 'medqa':\n",
    "         for example in ds:\n",
    "            prompt = f\"### Question: {example['input']} \\n ###Targets: {example['output']}\"\n",
    "            prompts.append(prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def select_characters_before_target(string):\n",
    "    target_phrase = \"### Targets:\"\n",
    "    target_index = string.find(target_phrase)\n",
    "    if target_index != -1:  # If the phrase is found\n",
    "        return string[:target_index] + target_phrase\n",
    "    else:\n",
    "        return string \n",
    "\n",
    "def group_examples_random(ds, n): #this is where we group examples into a larger prompt\n",
    "    samples = random.sample(ds, n)\n",
    "    new_prompt = \"\"\n",
    "    for i in range(n):\n",
    "        new_prompt += samples[i]\n",
    "    return new_prompt\n",
    "\n",
    "def group_by_similarity(prompt, ds, n_egs, m_choices):\n",
    "    choices = random.sample(ds, m_choices)\n",
    "    cos_sim_dict = {}\n",
    "    for c in choices:\n",
    "        cos_sim_dict[c] = sent_similarity(prompt, select_characters_before_target(c))\n",
    "\n",
    "    sorted_cos_sim = sorted(cos_sim_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_egs = \"\"\n",
    "    for item in sorted_cos_sim[:n_egs]:\n",
    "        top_egs += item[0]\n",
    "\n",
    "    top_egs = \"\".join([item[0] for item in sorted_cos_sim[:n_egs]])\n",
    "    return top_egs\n",
    "\n",
    "\n",
    "def evaluate_example(model, tokenizer, prompt):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    # print(prompt)\n",
    "    if len(tokenized_prompt['input_ids'][0]) > MAX_LENGTH: #currently just checking if random prompt is too big or not\n",
    "        return None \n",
    "    outputs =model.generate(**tokenized_prompt, pad_token_id=tokenizer.eos_token_id, max_length=MAX_LENGTH)\n",
    "    decoded_output = tokenizer.decode(outputs[0][len(tokenized_prompt['input_ids'][0]):], skip_special_tokens=True)\n",
    "    # print(\"prediction: \",decoded_output)\n",
    "    return decoded_output\n",
    "  \n",
    "\n",
    "def evaluate_icl(train_dataset, test_dataset, model, tokenizer, num_egs, ds_name='ni', method='similarity'):\n",
    "    reals = []\n",
    "    preds = []\n",
    "    for example in test_dataset:\n",
    "        # print(example)\n",
    "        # prompt = group_examples(train_dataset, num_egs) + f\"### Question: {example['input']} \\n ###Targets:\"\n",
    "        if ds_name == 'ni':\n",
    "            curr_prompt = f\"### Task: {example['definition']}\\n ### Inputs: {example['inputs']} \\n ### Targets:\"\n",
    "            real = f\"{example['targets']}\"\n",
    "        elif ds_name == 'medqa':\n",
    "            curr_prompt = f\"### Question: {example['input']} \\n ###Targets:\"\n",
    "            real = f\"{example['output']}\"\n",
    "        \n",
    "        if method == 'similarity':\n",
    "            icl_prompt = group_by_similarity(curr_prompt,train_dataset, num_egs, 100) + curr_prompt\n",
    "        elif method == 'random':\n",
    "            icl_prompt = group_examples_random(train_dataset, num_egs) + curr_prompt\n",
    "\n",
    "\n",
    "        # print(prompt)\n",
    "        # real = f\"{example['output']}\"\n",
    "        pred = evaluate_example(model, tokenizer, icl_prompt)\n",
    "        if pred:\n",
    "            reals.append(real)\n",
    "            preds.append(pred)\n",
    "\n",
    "    return reals, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models retrieved\n"
     ]
    }
   ],
   "source": [
    "max_num_egs = 3 #natural instructions are just too big\n",
    "\n",
    "model_plain =  GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer_plain = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(\"models retrieved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Natural Instructions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering done\n",
      "Length of test set:  10\n",
      "Length of train set 575481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score for ICL (1 eg) with 10 test cases:  tensor(0.7873)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score for ICL (2 eg) with 10 test cases:  tensor(0.7909)\n"
     ]
    }
   ],
   "source": [
    "data = load_from_disk('data/1000_per_task')\n",
    "\n",
    "# data = filter_icl(data, max_num_egs, tokenizer_plain)\n",
    "\n",
    "print(\"Filtering done\")\n",
    "\n",
    "\n",
    "# train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "# train_dataset = train_test_split['train']\n",
    "# test_dataset = train_test_split['test']\n",
    "\n",
    "train_dataset = data['train']\n",
    "test_dataset = data['test'].select(range(10))\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset)\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "for i in range(1, max_num_egs):\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_plain, tokenizer_plain, i, method='similarity')\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    print(f\"Average F1 score for ICL ({i} eg) with {len(preds)} test cases: \", average_F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    results_data.append({'num_egs' : len(preds), 'bert_score' : float(average_F1)})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv('icl_results_ni_similarity.csv', index=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(range(max_num_egs), bert_scores) \n",
    "# plt.xlabel('Number of examples') \n",
    "# plt.ylabel('BERT F1 Score') \n",
    "# plt.title('BERT F1 Score vs Number of Examples') \n",
    "# plt.xticks(range(max_num_egs))\n",
    "# plt.savefig('BERT_scores_icl_ni.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Medical QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set:  10\n",
      "Length of train set 8142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score for ICL (0 eg) with 10 test cases:  tensor(0.7270)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score for ICL (1 eg) with 10 test cases:  tensor(0.7703)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score for ICL (2 eg) with 10 test cases:  tensor(0.7694)\n"
     ]
    }
   ],
   "source": [
    "max_num_egs=3\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "data = load_dataset('medalpaca/medical_meadow_medqa')['train']\n",
    "train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test'].select(range(10))\n",
    "\n",
    "\n",
    "print(\"Length of test set: \", len(test_dataset))\n",
    "train_list = format_examples(train_dataset, ds_name='medqa')\n",
    "print(\"Length of train set\", len(train_list))\n",
    "\n",
    "bert_scores = []\n",
    "results_data = []\n",
    "for i in range(max_num_egs):\n",
    "    reals, preds = evaluate_icl(train_list, test_dataset, model_plain, tokenizer_plain, i, ds_name='medqa', method='random')\n",
    "    P, R, F1 = bert_score.score(preds, reals, lang=\"en\")\n",
    "    average_F1 = sum(F1) / len(F1)\n",
    "    print(f\"Average F1 score for ICL ({i} eg) with {len(preds)} test cases: \", average_F1)\n",
    "    bert_scores.append(average_F1)\n",
    "    bleu_score = bleu.compute(predictions=preds, references=reals)\n",
    "    results_data.append({'num_egs' : len(preds), 'bert_score' : float(average_F1), 'bleu_score' : bleu_score['bleu']})\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv('icl_results_med_mea_qa_random.csv', index=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(range(max_num_egs), bert_scores) \n",
    "# plt.xlabel('Number of examples') \n",
    "# plt.ylabel('BERT F1 Score')\n",
    "# plt.title('BERT F1 Score vs Number of Examples') \n",
    "# plt.xticks(range(max_num_egs))\n",
    "# plt.savefig('BERT_scores_icl_medqa.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
